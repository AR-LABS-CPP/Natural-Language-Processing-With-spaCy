{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b310388f",
   "metadata": {},
   "source": [
    "# Goals of this notebook\n",
    "- What is Text Feature Extraction\n",
    "- TF-IDF implementation from scratch\n",
    "- TF-IDF implementation using scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458d1c0b",
   "metadata": {},
   "source": [
    "## 1. Text Feature Extraction\n",
    "Text feature extraction in Natural Language Processing (NLP) refers to the process of transforming raw text data into numerical representations or features that machine learning models can understand and process. This is a crucial step in NLP pipelines as it allows the algorithms to learn from and make predictions based on text data.\n",
    "\n",
    "### 1.1 Count Vectorization\n",
    "Count Vectorization (also known as Count Vectors) is a method of converting text into numerical features by counting the occurrences of each word in the text. This method captures the frequency of each word within a document but does not account for word order or semantics.\n",
    "\n",
    "### 1.1.1 Tokenization:\n",
    "The text is split into individual words or tokens. For example, \"The cat sat on the mat\" would be tokenized into [\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"].\n",
    "\n",
    "### 1.1.2 Vocabulary Creation:\n",
    "A vocabulary is built from all unique tokens in the corpus. For instance, if the corpus consists of multiple documents, the vocabulary might include all unique words across all documents.\n",
    "\n",
    "### 1.1.3 Vectorization:\n",
    "Each document is represented as a vector where each dimension corresponds to a word in the vocabulary. The value in each dimension is the count of occurrences of that word in the document.\n",
    "For example, if the vocabulary is [\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"], the document \"The cat sat on the mat\" might be represented as [2, 1, 1, 1, 1, 1], where the first dimension corresponds to \"The\", and the count is 2 because \"The\" appears twice.\n",
    "\n",
    "### 1.1.4 Example:\n",
    "For a small corpus with two documents:\n",
    "Document 1: \"The cat sat on the mat\"\n",
    "Document 2: \"The cat is on the mat\"\n",
    "The vocabulary might be [\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\", \"is\"]. Notice that \"The\" and \"the\" are considered different due to case sensitivity.\n",
    "\n",
    "The Count Vectors might look like:\n",
    "\n",
    "Document 1: [2, 1, 1, 1, 1, 1, 0]\n",
    "Document 2: [2, 1, 0, 1, 1, 1, 1]\n",
    "\n",
    "## 1.2 Document-Term Matrix (DTM)\n",
    "The Document-Term Matrix (DTM) is a tabular representation of the text data where rows represent documents and columns represent terms (words). Each cell in the matrix indicates the count of a specific term in a specific document.\n",
    "\n",
    "### 1.2.1 Constructing the Matrix:\n",
    "Each row corresponds to a document.\n",
    "Each column corresponds to a term in the vocabulary.\n",
    "The cell value at position (i, j) represents the count of term j in document i.\n",
    "\n",
    "### 1.2.3 Matrix Representation:\n",
    "The DTM is essentially a sparse matrix because most of its values are zero (especially in large vocabularies).\n",
    "Example:\n",
    "Using the same documents as above:\n",
    "\n",
    "The\tcat\tsat\ton\tthe\tmat\tis:\n",
    "- Document 1\t`2\t1\t1\t1\t1\t1\t0`\n",
    "- Document 2\t`2\t1\t0\t1\t1\t1\t1`\n",
    "<br>\n",
    "\n",
    "Here, each cell represents the count of the corresponding term in the respective document.\n",
    "\n",
    "## 1.3 Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "It is a numerical statistic used in text processing to evaluate the importance of a word within a document relative to a collection of documents (corpus). It combines two metrics: Term Frequency (TF) and Inverse Document Frequency (IDF).\n",
    "\n",
    "### 1.3.1 Term Frequency (TF)\n",
    "Term Frequency measures how frequently a term occurs in a document. The simplest form of TF is:\n",
    "$$ \\text{TF}(t, d) = \\frac{\\text{Number of times term } t \\text{ appears in document } d}{\\text{Total number of terms in document } d} $$\n",
    "\n",
    "Where:\n",
    "- t is the term (word).\n",
    "- d is the document.\n",
    "\n",
    "TF provides a basic measure of how important a term is within a specific document, but it doesn’t account for the term’s importance across the entire corpus.\n",
    "\n",
    "### 1.3.2 Inverse Document Frequency (IDF)\n",
    "Inverse Document Frequency measures the importance of a term across the whole corpus. It helps to adjust for the fact that some words are very common and may not be very informative. The IDF of a term is calculated as:\n",
    "$$ \\text{IDF}(t, D) = \\log \\frac{\\text{Total number of documents } |D|}{\\text{Number of documents containing term } t} $$\n",
    "\n",
    "Where:\n",
    "- ∣D∣ is the total number of documents in the corpus.\n",
    "- The denominator is the count of documents that contain the term \n",
    "\n",
    "The IDF decreases the weight of terms that occur very frequently across many documents (like “the”, “is”, etc.), making them less informative.\n",
    "\n",
    "### 1.3.3 TF-IDF Calculation\n",
    "The TF-IDF score for a term in a document is the product of its TF and IDF values:\n",
    "$$ \\text{TF-IDF}(t, d, D) = \\text{TF}(t, d) \\times \\text{IDF}(t, D) $$\n",
    "\n",
    "This score reflects both the term's importance in the specific document and its rarity across the corpus.\n",
    "\n",
    "### 1.3.4 TF-DF Calculation Example\n",
    "Let's go through an example with a small corpus:\n",
    "\n",
    "Corpus:\n",
    "- \"The cat sat on the mat\"\n",
    "- \"The cat is on the mat\"\n",
    "- \"The dog sat on the log\"\n",
    "\n",
    "**Term Frequency (TF):**\n",
    "\n",
    "For the term “cat” in Document 1:\n",
    "\n",
    "$$ \\text{TF}(\\text{cat}, \\text{Document 1}) = \\frac{1}{6} $$\n",
    "\n",
    "(1 occurrence out of 6 total words)\n",
    "\n",
    "For the term “cat” in Document 2:\n",
    "\n",
    "$$ \\text{TF}(\\text{cat}, \\text{Document 2}) = \\frac{1}{6} $$\n",
    "\n",
    "(1 occurrence out of 6 total words)\n",
    "\n",
    "**Inverse Document Frequency (IDF):**\n",
    "\n",
    "- Total number of documents \\( |D| = 3 \\)\n",
    "- Number of documents containing “cat” = 2\n",
    "\n",
    "IDF for “cat”:\n",
    "\n",
    "$$ \\text{IDF}(\\text{cat}, D) = \\log \\frac{|D|}{\\text{Number of documents containing term } \\text{cat}} = \\log \\frac{3}{2} \\approx 0.176 $$\n",
    "\n",
    "**TF-IDF Calculation:**\n",
    "\n",
    "For “cat” in Document 1:\n",
    "\n",
    "$$ \\text{TF-IDF}(\\text{cat}, \\text{Document 1}, D) = \\text{TF}(\\text{cat}, \\text{Document 1}) \\times \\text{IDF}(\\text{cat}, D) = \\frac{1}{6} \\times 0.176 \\approx 0.029 $$\n",
    "\n",
    "For “cat” in Document 2:\n",
    "\n",
    "$$ \\text{TF-IDF}(\\text{cat}, \\text{Document 2}, D) = \\text{TF}(\\text{cat}, \\text{Document 2}) \\times \\text{IDF}(\\text{cat}, D) = \\frac{1}{6} \\times 0.176 \\approx 0.029 $$\n",
    "\n",
    "In this example, the TF-IDF score for \"cat\" in both documents is the same. If \"cat\" appeared in fewer documents, its IDF would be higher, and thus its TF-IDF score would be higher as well, reflecting its greater importance in those specific documents.\n",
    "\n",
    "Fortunately, all of these calculations can be performed through Scikit-Learn but we will also implement it from scratch as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ff1363",
   "metadata": {},
   "source": [
    "## 2. TF-DF implementation from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc5c95f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57a50aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we only want the spacy tokenizer, so disable everything else\n",
    "nlp = spacy.load(\n",
    "    \"en_core_web_md\",\n",
    "    disable=[\"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\", \"ner\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8933ea7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4fafa02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_remove_punkt(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.text for token in doc if not token.is_punct]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6a31c9",
   "metadata": {},
   "source": [
    "The dataset is taken from here [Medium Articles Kaggle](https://www.kaggle.com/datasets/hsankesara/medium-articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef8be447",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./medium_articles.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67866aad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>claps</th>\n",
       "      <th>reading_time</th>\n",
       "      <th>link</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Justin Lee</td>\n",
       "      <td>8.3K</td>\n",
       "      <td>11</td>\n",
       "      <td>https://medium.com/swlh/chatbots-were-the-next...</td>\n",
       "      <td>Chatbots were the next big thing: what happene...</td>\n",
       "      <td>Oh, how the headlines blared:\\nChatbots were T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Conor Dewey</td>\n",
       "      <td>1.4K</td>\n",
       "      <td>7</td>\n",
       "      <td>https://towardsdatascience.com/python-for-data...</td>\n",
       "      <td>Python for Data Science: 8 Concepts You May Ha...</td>\n",
       "      <td>If you’ve ever found yourself looking up the s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>William Koehrsen</td>\n",
       "      <td>2.8K</td>\n",
       "      <td>11</td>\n",
       "      <td>https://towardsdatascience.com/automated-featu...</td>\n",
       "      <td>Automated Feature Engineering in Python – Towa...</td>\n",
       "      <td>Machine learning is increasingly moving from h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gant Laborde</td>\n",
       "      <td>1.3K</td>\n",
       "      <td>7</td>\n",
       "      <td>https://medium.freecodecamp.org/machine-learni...</td>\n",
       "      <td>Machine Learning: how to go from Zero to Hero ...</td>\n",
       "      <td>If your understanding of A.I. and Machine Lear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Emmanuel Ameisen</td>\n",
       "      <td>935</td>\n",
       "      <td>11</td>\n",
       "      <td>https://blog.insightdatascience.com/reinforcem...</td>\n",
       "      <td>Reinforcement Learning from scratch – Insight ...</td>\n",
       "      <td>Want to learn about applied Artificial Intelli...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             author claps  reading_time  \\\n",
       "0        Justin Lee  8.3K            11   \n",
       "1       Conor Dewey  1.4K             7   \n",
       "2  William Koehrsen  2.8K            11   \n",
       "3      Gant Laborde  1.3K             7   \n",
       "4  Emmanuel Ameisen   935            11   \n",
       "\n",
       "                                                link  \\\n",
       "0  https://medium.com/swlh/chatbots-were-the-next...   \n",
       "1  https://towardsdatascience.com/python-for-data...   \n",
       "2  https://towardsdatascience.com/automated-featu...   \n",
       "3  https://medium.freecodecamp.org/machine-learni...   \n",
       "4  https://blog.insightdatascience.com/reinforcem...   \n",
       "\n",
       "                                               title  \\\n",
       "0  Chatbots were the next big thing: what happene...   \n",
       "1  Python for Data Science: 8 Concepts You May Ha...   \n",
       "2  Automated Feature Engineering in Python – Towa...   \n",
       "3  Machine Learning: how to go from Zero to Hero ...   \n",
       "4  Reinforcement Learning from scratch – Insight ...   \n",
       "\n",
       "                                                text  \n",
       "0  Oh, how the headlines blared:\\nChatbots were T...  \n",
       "1  If you’ve ever found yourself looking up the s...  \n",
       "2  Machine learning is increasingly moving from h...  \n",
       "3  If your understanding of A.I. and Machine Lear...  \n",
       "4  Want to learn about applied Artificial Intelli...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6a3b69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create vocab and tokenize the docs\n",
    "vocab = {}\n",
    "idx = 0\n",
    "tokenized_docs = []\n",
    "\n",
    "for doc in df[\"text\"]:\n",
    "    tokens = tokenize_and_remove_punkt(doc.lower())\n",
    "    doc_tokens = []\n",
    "    \n",
    "    for token in tokens:\n",
    "        if token not in vocab:\n",
    "            vocab[token] = idx\n",
    "            idx += 1\n",
    "        \n",
    "        doc_tokens.append(vocab[token])\n",
    "    \n",
    "    tokenized_docs.append(doc_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "efb6aa13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reverse mapping (index to word)\n",
    "words = [key for key in vocab.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0eccff28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# N = no of documents, V = size of vocabulary\n",
    "N = len(df[\"text\"])\n",
    "V = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8fe580ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# term frequency matrix (dense)\n",
    "term_freq = np.zeros((N, V))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355ffcce",
   "metadata": {},
   "source": [
    "Recall, that every cell in the term frequency matrix represents the occurrence of a word for a specific document.\n",
    "- row represents the document e.g. document 1, document 2\n",
    "- col represents the word itself (in tokenized form)\n",
    "\n",
    "### Term Frequency Matrix\n",
    "\n",
    "|       | this | is | a  | sample | sentence | another | example | different |\n",
    "|-------|------|----|----|--------|----------|---------|---------|-----------|\n",
    "| Doc 1 | 1    | 1  | 1  | 1      | 1        | 0       | 0       | 0         |\n",
    "| Doc 2 | 1    | 1  | 0  | 0      | 1        | 1       | 1       | 0         |\n",
    "| Doc 3 | 1    | 1  | 0  | 0      | 0        | 0       | 1       | 1         |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e914d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill the term frequency matrix with the occurrence of words\n",
    "for doc_idx, tokenized_doc in enumerate(tokenized_docs):\n",
    "    for token_idx in tokenized_doc:\n",
    "        term_freq[doc_idx, token_idx] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f455a553",
   "metadata": {},
   "source": [
    "The line `doc_freq = np.sum(term_freq > 0, axis=0)` counts the occurrence of a specific word in all documents, for example in the above table \"this\" appears in all the document (hence it's document frequency is 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c9ec5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate IDF (inverse document frequency)\n",
    "doc_freq = np.sum(term_freq > 0, axis=0)\n",
    "\n",
    "# numpy will automatically broadcast i.e. divide each doc_freq value with N\n",
    "idf = np.log(N / doc_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee6e78a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.87564395, 0.        , 0.01494796, ..., 5.82008293, 5.82008293,\n",
       "       5.82008293])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here each value in the array represents the idf of the word\n",
    "idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "269e8f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# each document row will be multiplied with the idf vector\n",
    "tf_idf = term_freq * idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "851eead4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label:  Cheat Sheets for AI, Neural Networks, Machine Learning, Deep Learning & Big Data\n",
      "\n",
      "\n",
      "Starting text:  Over the past few months, I have been collecting AI cheat sheets. From time to time I share them with friends and colleagues and recently I have been getting asked a lot, so I decided to organize and share the entire collection. To make things more interesting and give context, I added descriptions and/or excerpts for each major topic.\n",
      "\n",
      "\n",
      "Top ten words:  ['cheat', 'sheet', 'numpy', 'matplotlib', 'scipy', 'pandas', 'matlab', 'tpus', 'chatbot', 'scikit', 'keras']\n"
     ]
    }
   ],
   "source": [
    "# let's test this out\n",
    "random_idx = np.random.choice(N)\n",
    "row = df.iloc[random_idx]\n",
    "\n",
    "print(\"Label: \", row[\"title\"].split(\"\\n\", 1)[0])\n",
    "print(\"\\n\")\n",
    "print(\"Starting text: \", row[\"text\"].split(\"\\n\", 1)[0])\n",
    "\n",
    "scores = tf_idf[random_idx]\n",
    "top_ten = (-scores).argsort()[:11]\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Top ten words: \", [words[idx] for idx in top_ten])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1d2d56",
   "metadata": {},
   "source": [
    "## 3. Using sparse matrix from Scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31999748",
   "metadata": {},
   "source": [
    "Sparse matrices are memory efficient since they only store non-zero elements, reducing memory usage. This is particularly beneficial for large matrices with few non-zero elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cd5b7bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fae30b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for scipy the method is a little different\n",
    "data = []\n",
    "rows = []\n",
    "cols = []\n",
    "\n",
    "for doc_idx, tokenized_doc in enumerate(tokenized_docs):\n",
    "    term_counts = defaultdict(int)\n",
    "    \n",
    "    for token_idx in tokenized_doc:\n",
    "        term_counts[token_idx] += 1\n",
    "    \n",
    "    for token_idx, count in term_counts.items():\n",
    "        data.append(count)\n",
    "        rows.append(doc_idx)\n",
    "        cols.append(token_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b3aaa988",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_term_freq = csr_matrix((data, (rows, cols)), shape=(N, V))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e1523668",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_term_freq = (sparse_term_freq > 0).astype(int)\n",
    "\n",
    "# sum along the cols and convert to 1D array by flattening it\n",
    "document_freq = np.array(binary_term_freq.sum(axis=0)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "03c429f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = np.log(N / document_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e2fcd519",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = sparse_term_freq.multiply(idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3a4e9e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label:  TensorFlow Tutorial— Part 1 – Illia Polosukhin – Medium\n",
      "\n",
      "\n",
      "Starting text:  UPD (April 20, 2016): Scikit Flow has been merged into TensorFlow since version 0.8 and now called TensorFlow Learn or tf.learn.\n",
      "\n",
      "\n",
      "Top five words:  ['scikit', 'tf.learn', 'tensorflow', 'flow', 'ipython', 'dataset']\n"
     ]
    }
   ],
   "source": [
    "rand_idx = np.random.choice(N)\n",
    "row = df.iloc[rand_idx]\n",
    "\n",
    "print(\"Label: \", row[\"title\"].split(\"\\n\", 1)[0])\n",
    "print(\"\\n\")\n",
    "print(\"Starting text: \", row[\"text\"].split(\"\\n\", 1)[0])\n",
    "\n",
    "scores = tf_idf.getrow(rand_idx).toarray().flatten()\n",
    "top_five = (-scores).argsort()[:6]\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Top five words: \", [words[idx] for idx in top_five])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581a12f4",
   "metadata": {},
   "source": [
    "## 4. Covert everything to a neat class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "64c0b616",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import spacy\n",
    "from collections import defaultdict\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "class TFIDF:\n",
    "    def __init__(self):\n",
    "        self.vocab = {}\n",
    "        self.tokenized_docs = []\n",
    "        self.reverseMap = []\n",
    "        self.sparse_matrix = None\n",
    "        self.tf_idf = None\n",
    "        self.nlp = spacy.load(\n",
    "            \"en_core_web_md\",\n",
    "            disable=[\"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\", \"ner\"]\n",
    "        )\n",
    "        \n",
    "    def tokenize_and_remove_punkt(self, text):\n",
    "        doc = self.nlp(text)\n",
    "        tokens = [token.text for token in doc if not token.is_punct]\n",
    "        return tokens\n",
    "        \n",
    "    def create_vocabulary(self, documents):\n",
    "        idx = 0\n",
    "        \n",
    "        for doc in documents:\n",
    "            tokens = self.tokenize_and_remove_punkt(doc.lower())\n",
    "            doc_tokens = []\n",
    "\n",
    "            for token in tokens:\n",
    "                if token not in self.vocab:\n",
    "                    self.vocab[token] = idx\n",
    "                    idx += 1\n",
    "\n",
    "                doc_tokens.append(self.vocab[token])\n",
    "\n",
    "            self.tokenized_docs.append(doc_tokens)\n",
    "        \n",
    "        self.reverseMap = [key for key in self.vocab.keys()]\n",
    "        \n",
    "    def create_sparse_matrix(self):\n",
    "        data = []\n",
    "        rows = []\n",
    "        cols = []\n",
    "\n",
    "        for doc_idx, tokenized_doc in enumerate(tokenized_docs):\n",
    "            term_counts = defaultdict(int)\n",
    "\n",
    "            for token_idx in tokenized_doc:\n",
    "                term_counts[token_idx] += 1\n",
    "\n",
    "            for token_idx, count in term_counts.items():\n",
    "                data.append(count)\n",
    "                rows.append(doc_idx)\n",
    "                cols.append(token_idx)\n",
    "                \n",
    "        self.sparse_matrix = csr_matrix((data, (rows, cols)), shape=(N, V))\n",
    "        \n",
    "    def calculate_tf_idf(self):\n",
    "        binary_term_freq = (sparse_term_freq > 0).astype(int)\n",
    "        document_freq = np.array(binary_term_freq.sum(axis=0)).flatten()\n",
    "        idf = np.log(N / document_freq)\n",
    "        \n",
    "        self.tf_idf = self.sparse_matrix.multiply(idf)\n",
    "        \n",
    "    def fit_transform(self, documents):\n",
    "        self.create_vocabulary(documents)\n",
    "        self.create_sparse_matrix()\n",
    "        self.calculate_tf_idf()\n",
    "        \n",
    "        return self.tf_idf\n",
    "    \n",
    "    def get_feature_names(self):\n",
    "        return self.reverseMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ebc4c5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TFIDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dcc0d939",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = tfidf_vectorizer.fit_transform(df[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e3b487a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b96dd21a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label:  A Rock Album For AI – Carlos Beltran – Medium\n",
      "\n",
      "\n",
      "Starting text:  https://open.spotify.com/album/0jwnYwJz6XHNrVAYEclQPd\n",
      "\n",
      "\n",
      "Top five words:  ['album', 'song', 'kurzweil', 'simulation', 'tim', 'musk']\n"
     ]
    }
   ],
   "source": [
    "rand_idx = np.random.choice(N)\n",
    "row = df.iloc[rand_idx]\n",
    "\n",
    "print(\"Label: \", row[\"title\"].split(\"\\n\", 1)[0])\n",
    "print(\"\\n\")\n",
    "print(\"Starting text: \", row[\"text\"].split(\"\\n\", 1)[0])\n",
    "\n",
    "scores = matrix.getrow(rand_idx).toarray().flatten()\n",
    "top_five = (-scores).argsort()[:6]\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Top five words: \", [feature_names[idx] for idx in top_five])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0e1d9c",
   "metadata": {},
   "source": [
    "## 5. TF-IDF using scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "efcd5c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cb1a45e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_df=0.9,\n",
    "    min_df=0.1,\n",
    "    max_features=2000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9e0e7abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_mat = tfidf_vectorizer.fit_transform(df[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fa884376",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<337x1280 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 109373 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c6eac975",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = tfidf_vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "46afed13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label:  What Are The Best Intelligent Chatbots or AI Chatbots Available Online?\n",
      "\n",
      "\n",
      "Starting text:  How do we define the intelligence of a chatbot? You can see a lot of articles about what would make a chatbot “appear intelligent.” A chatbot is intelligent when it becomes aware of user needs. Its intelligence is what gives the chatbot the ability to handle any scenario of a conversation with ease.\n",
      "\n",
      "\n",
      "Top ten words:  ['bot', 'intelligent', 'conversation', 'most', 'click', 'ai']\n"
     ]
    }
   ],
   "source": [
    "rand_idx = np.random.choice(tfidf_mat.shape[0])\n",
    "row = df.iloc[rand_idx]\n",
    "\n",
    "print(\"Label: \", row[\"title\"].split(\"\\n\", 1)[0])\n",
    "print(\"\\n\")\n",
    "print(\"Starting text: \", row[\"text\"].split(\"\\n\", 1)[0])\n",
    "\n",
    "tfidf_vector = tfidf_mat.getrow(rand_idx).toarray().flatten()\n",
    "\n",
    "top_indices = (-tfidf_vector).argsort()[:6]\n",
    "\n",
    "top_words = [feature_names[i] for i in top_indices]\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Top ten words: \", top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5db7a49",
   "metadata": {},
   "source": [
    "## The End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
