{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14fbefed",
   "metadata": {},
   "source": [
    "# Goals of this notebook\n",
    "- Definition & concepts related to markov models\n",
    "- Markov chain implementation from scratch\n",
    "- Markov chain implementation using Markovify"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfe2f49",
   "metadata": {},
   "source": [
    "## 1. What is a Markov chain?\n",
    "A Markov chain, or Markov process, is a stochastic model that describes a sequence of events where the probability of each event depends solely on the state attained in the previous event. This sequence of events is often referred to as a **state sequence**. The key characteristic of a Markov chain is that it exhibits the **Markov property**.\n",
    "\n",
    "Markov chains are widely applied in various fields, including finance, reinforcement learning, and biological sequence analysis, to model probabilistic processes and predict future states based on current observations.\n",
    "\n",
    "### 1.1 Markov Property\n",
    "The Markov property is a fundamental characteristic of a Markov chain, indicating that the future state of the process depends only on the present state, not on the sequence of events that preceded it. This property is often described as ***memorylessness** or **lack of history**.\n",
    "\n",
    "Recall, that a sequence is just a ordered list of values or symbols.\n",
    "<br>\n",
    "\n",
    "$$ \\text{Sequence} = \\{x_1, x_2,\\ldots,x_T\\} $$\n",
    "\n",
    "A common and intuitive question to ask is, what is the probability of a sequence, that is, given a sequence $ x_1 $ to $ x_T $, what is the probability of seeing $ x_1 $ upto $ x_T $. More formally:\n",
    "\n",
    "$$ \\text{Probability of sequence} = p(x_1, x_2,\\ldots,x_T) $$\n",
    "\n",
    "Using this, we can calculate all sorts of useful things. For example, if we only know the values from $ x_1 $ upto $ x_{T - 1} $. We can calculate the distribtion of $ x_T $, given all the previous values.\n",
    "\n",
    "$$ \\text{Next} = p(x_T \\mid x_{T - 1}, x_{T - 2},\\ldots,x_1) $$\n",
    "\n",
    "This can be useful for forecasting the value in a time series or for predicting the next word in a sequence.\n",
    "\n",
    "As stated earlier, Markov property is restrictive in the sense that it asserts that the future state of the process is dependent only on the current state and not on the sequence of states that preceded it. This means that given the present state, the future evolution of the process is independent of the past states.\n",
    "\n",
    "![Markov Chain](https://www.dropbox.com/scl/fi/ijoacf2pjhs4dvdr018j0/Markov-chain.svg?rlkey=84sohxm9si13h9zy8pshek95z&st=gr7hyx6n&raw=1)\n",
    "\n",
    "According to Markov property, the joint probability can be decomposed into the product of the initial state probability and the conditional probabilities of each subsequent state given the previous state.\n",
    "<br>\n",
    "\n",
    "$$ p(x_1, \\ldots, x_T) = p(x_1) \\prod_{t=2}^{T} p(x_t \\mid x_{t-1}) $$\n",
    "\n",
    "where:\n",
    "- $ p(x_1) $ is the probability of the initial state.\n",
    "- $ p(x_t \\mid x_{T - 1}) $ transition probabilities from one state to the next.\n",
    "\n",
    "The symbol $ x_1 $ is not conditioned on anything since it is assumed that nothing comes before $ x_1 $.\n",
    "\n",
    "### 1.2 Chain Rule of Probability\n",
    "The chain rule of probability provides a way to express the joint probability of a sequence of random variables as a product of conditional probabilities. It decomposes the joint distribution into simpler conditional probabilities, making it easier to handle complex probability calculations.\n",
    "\n",
    "For a sequence of random variables $ ( x_1, x_2, \\ldots, x_T ) $, the chain rule states that each term depends on all preceding terms.\n",
    "\n",
    "$$ p(x_1, x_2, \\ldots, x_T) = p(x_1) \\cdot p(x_2 \\mid x_1) \\cdot p(x_3 \\mid x_1, x_2) \\cdot \\ldots \\cdot p(x_T \\mid x_1, x_2, \\ldots, x_{T-1}) $$\n",
    "\n",
    "## 2. Markov Model\n",
    "A Markov model is a statistical model that describes a system which transitions from one state to another within a finite set of states. The model is defined by a set of states and the probabilities of moving from one state to another. These probabilities are typically represented in a matrix called the transition matrix.\n",
    "\n",
    "Suppose we want to model the weather, where the weather can be either **Sunny** or **Rainy** each day. We can use a markov model to predict the weather for the next day based on the current weather.\n",
    "\n",
    "Let **S** represent the states and let **P** represent the transition matrix. Now assume that after some observation we got the following probabilities.\n",
    "\n",
    "- If it is Sunny today, there is a **0.8** probability that it will be Sunny tomorrow and a **0.2** probability that it will be Rainy tomorrow.\n",
    "- If it is Rainy today, there is a **0.6** probability that it will be Rainy tomorrow and a **0.4** probability that it will be Sunny tomorrow.\n",
    "\n",
    "Then the transition matrix will be represented as follows:\n",
    "\n",
    "|       | Sunny | Rainy\n",
    "|-------|------|----|\n",
    "| Sunny | 0.8    | 0.2  |\n",
    "| Rainy | 0.4    | 0.6  |\n",
    "\n",
    "### 2.1 Notations\n",
    "- A state in markov models is represented by **s** or **x** or **z**.\n",
    "- Time is represented by **t**. Time is assumed to be discrete (no floating point values).\n",
    "\n",
    "$$ s(t) = s_t = \\text{state at time}\\; t $$\n",
    "\n",
    "- Total number of states is represented by $ N $.\n",
    "- The transition matrix **P** is a matrix where $ P_{ij} $ is the probability of transitioning from state $ i $ to state $ j $.\n",
    "- $ P_{ij} $ is the probability of transitioning from state $ i $ to state $ j $.\n",
    "- $ P_{ij} $ is often written as Often written as $ P(s_{t + 1} = j \\mid s_{t} = i) $. This reads as \"Probability that state at time **t** is **j**, given that the state at time **t - 1** was **i**.\n",
    "\n",
    "### 2.2 Initial state\n",
    "To quantify the probability of the first state in a sequence, we use the initial state distribution. This is essential because the first state in a Markov process does not have a preceding state that influences its probability, unlike subsequent states that depend on the transitions from earlier states.\n",
    "\n",
    "The initial state distribution, often denoted as $ \\pi $ or $ \\pi_{0} $, specifies the probability of each possible state being the starting state of the process. It provides a probability distribution over all possible states at the beginning of the Markov process (i.e. at time $ t = 0 $).\n",
    "\n",
    "### 2.3 Calculating transition matrix and initial state vector\n",
    "The calculation is carried out using the following formulas.\n",
    "\n",
    "For calculating the initial state vector.\n",
    "\n",
    "$$ \\hat{\\pi_{i}} = \\frac{\\text{count} (s_1 = i)}{N} $$\n",
    "\n",
    "Here $ N $ is the number of sequences in the dataset and $ \\text{count} (s_1 = i) $ is the number of times each sequence started with state $ i $.\n",
    "\n",
    "For calculating the transition matrix.\n",
    "\n",
    "$$ \\hat{P{ij}} = \\frac{\\text{count} (i \\rightarrow j)}{\\text{count}(i)} $$\n",
    "\n",
    "Here $ \\text{count} (i \\rightarrow j) $ is the number of transitions observed from state $ i $ to state $ j $ in the data. $ \\text{count}(i) $ represents the total number of times the system was in state $ i $ during the observation period.\n",
    "\n",
    "## 3. N-Order Markov Model for Text Generation\n",
    "In the context of text generation, a Markov model can be extended to consider not just the current state but also the previous $ N - 1 $ states. This is known as an $ N $ order markov model. Unlike a first-order Markov model, where the next state depends only on the current state, an $ N $-order markov model incorporates information from the previous $ N - 1 $ states to determine the probability of the next state.\n",
    "\n",
    "An $ N $-order markov model, also referred to as an N-gram model in natural language processing, can be defined as follows:\n",
    "\n",
    "### 3.1 Notation\n",
    "  - Let $ ( w_t ) $ denote the word at position $ ( t ) $.\n",
    "  - The probability of generating a word $ ( w_t ) $ given the previous $ ( N-1 ) $ words $ ( w_{t-1}, w_{t-2}, \\ldots, w_{t-N+1} ) $ is:\n",
    "\n",
    "$$ P(w_t \\mid w_{t-1}, w_{t-2}, \\ldots, w_{t-N+1}) $$\n",
    "\n",
    "### 3.2 Transition Probabilities in N-Order Markov Models\n",
    "In an N-order Markov model, the transition probability is conditioned on the previous $ ( N-1 ) $ states. The transition probabilities can be estimated from a corpus of text data as follows:\n",
    "\n",
    "- **Count Occurrences**: Count how often a specific sequence of $ ( N-1 ) $ words precedes a word $ ( w_t ) $.\n",
    "- **Estimate Probabilities**: Calculate the probability of a word $ ( w_t ) $ following a sequence of $ ( N-1 ) $ words by dividing the count of the sequence $ ( w_{t-N+1}, \\ldots, w_{t-1}, w_t ) $ by the count of the sequence $ ( w_{t-N+1}, \\ldots, w_{t-1} ) $.\n",
    "\n",
    "$$ \\hat{P}(w_t \\mid w_{t-1}, w_{t-2}, \\ldots, w_{t-N+1}) = \\frac{\\text{count}(w_{t-N+1}, \\ldots, w_{t-1}, w_t)}{\\text{count}(w_{t-N+1}, \\ldots, w_{t-1})} $$\n",
    "\n",
    "### 3.3 Example of an N-Order Markov Model\n",
    "\n",
    "To illustrate how an N-order Markov model works, let's use a bigram (2-order Markov model) for text generation. In this model, each word depends on the previous word.\n",
    "\n",
    "#### Corpus\n",
    "Suppose that we have loaded the corpus and tokenized it, after tokenizing we have generated the following bigrams.\n",
    "- \"the cat\"\n",
    "- \"cat sat\"\n",
    "- \"sat on\"\n",
    "- \"on the\"\n",
    "- \"the mat\"\n",
    "\n",
    "#### Transition Probabilities\n",
    "From these sequences, we can estimate the transition probabilities. Let's break down the calculations step-by-step:\n",
    "\n",
    "- **List of bigrams**:\n",
    "   - \"the cat\"\n",
    "   - \"cat sat\"\n",
    "   - \"sat on\"\n",
    "   - \"on the\"\n",
    "   - \"the mat\"\n",
    "   \n",
    "   <br>\n",
    "- **Count occurrences of each bigram and each word**:\n",
    "   - \"the\" appears 2 times (as a starting word in \"the cat\" and \"the mat\")\n",
    "   - \"cat\" appears 1 time (as a starting word in \"cat sat\")\n",
    "   - \"sat\" appears 1 time (as a starting word in \"sat on\")\n",
    "   - \"on\" appears 1 time (as a starting word in \"on the\")\n",
    "   \n",
    "   <br>\n",
    "- **Count occurrences of transitions**:\n",
    "   - $ P(\\text{\"sat\"} \\mid \\text{\"cat\"}) $ = \"cat sat\" appears 1 time.\n",
    "   - $ P(\\text{\"on\"} \\mid \\text{\"sat\"}) $ = \"sat on\" appears 1 time.\n",
    "   - $ P(\\text{\"the\"} \\mid \\text{\"on\"}) $ = \"on the\" appears 1 time.\n",
    "   - $ P(\\text{\"cat\"} \\mid \\text{\"the\"}) $ = \"the cat\" appears 1 time.\n",
    "   - $ P(\\text{\"mat\"} \\mid \\text{\"the\"}) $ = \"the mat\" appears 1 time.\n",
    "   \n",
    "   <br>\n",
    "- **Calculate transition probabilities**:\n",
    "   - $ P(\\text{\"cat\"} \\mid \\text{\"the\"}) = \\frac{\\text{count}(\\text{\"the cat\"})}{\\text{count}(\\text{\"the\"})} = \\frac{1}{2} = 0.5 $\n",
    "   - $ P(\\text{\"mat\"} \\mid \\text{\"the\"}) = \\frac{\\text{count}(\\text{\"the mat\"})}{\\text{count}(\\text{\"the\"})} = \\frac{1}{2} = 0.5 $\n",
    "   - $ P(\\text{\"sat\"} \\mid \\text{\"cat\"}) = \\frac{\\text{count}(\\text{\"cat sat\"})}{\\text{count}(\\text{\"cat\"})} = \\frac{1}{1} = 1.0 $\n",
    "   - $ P(\\text{\"on\"} \\mid \\text{\"sat\"}) = \\frac{\\text{count}(\\text{\"sat on\"})}{\\text{count}(\\text{\"sat\"})} = \\frac{1}{1} = 1.0 $\n",
    "   - $ P(\\text{\"the\"} \\mid \\text{\"on\"}) = \\frac{\\text{count}(\\text{\"on the\"})}{\\text{count}(\\text{\"on\"})} = \\frac{1}{1} = 1.0 $\n",
    "   \n",
    "   <br>\n",
    "- **Transition matrix**:\n",
    "   - From \"the\": 50% chance of \"cat\", 50% chance of \"mat\".\n",
    "   - From \"cat\": 100% chance of \"sat\".\n",
    "   - From \"sat\": 100% chance of \"on\".\n",
    "   - From \"on\": 100% chance of \"the\".\n",
    "\n",
    "#### Generating Text\n",
    "Now, let’s generate text using the estimated transition probabilities:\n",
    "\n",
    "- **Start with an initial word**: Let's start with \"the\".\n",
    "- **Use transition probabilities to choose the next word**:\n",
    "   - From \"the\", randomly choose between \"cat\" (50%) and \"mat\" (50%). Suppose we choose \"cat\".\n",
    "   - From \"cat\", \"sat\" is the only option.\n",
    "   - From \"sat\", \"on\" is the only option.\n",
    "   - From \"on\", \"the\" is the only option.\n",
    "\n",
    "- **Append the chosen word to the sequence and repeat**:\n",
    "   - Starting with \"the\", we chose \"cat\" → \"the cat\".\n",
    "   - Next word after \"cat\" is \"sat\" → \"the cat sat\".\n",
    "   - Next word after \"sat\" is \"on\" → \"the cat sat on\".\n",
    "   - Next word after \"on\" is \"the\" → \"the cat sat on the\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ea7d98",
   "metadata": {},
   "source": [
    "## 4. Markov Model Implementation from scratch\n",
    "You can find the pseudocode for the algorithm on the following site: [Markov Chain Algorithm](https://www.rose-hulman.edu/class/csse/csse221/200910/Projects/Markov/markov.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f73536cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "from collections import defaultdict\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f07e3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarkovChain:\n",
    "    def __init__(self, n):\n",
    "        self.n = n\n",
    "        self.prefixes = defaultdict(list)\n",
    "        self.suffix_counts = defaultdict(lambda: defaultdict(int))\n",
    "        self.boundary = \"NONWORD\"\n",
    "    \n",
    "    def build_model(self, text):\n",
    "        words = [word for line in text for word in line]\n",
    "        \n",
    "        # add boundaries e.g. \"[\"NOWORD\", \"NOWORD\", words, \"NOWORD\"]\"\n",
    "        words = [self.boundary] * self.n + words + [self.boundary]\n",
    "        \n",
    "        # loop and skip the last n tokens\n",
    "        for i in range(len(words) - self.n):\n",
    "            # prefix will contain first two words of sequence (word_one, word_two)\n",
    "            prefix = tuple(words[i:i + self.n])\n",
    "            \n",
    "            # suffix will contain the thrid word of the sequence\n",
    "            suffix = words[i + self.n]\n",
    "            \n",
    "            # prefixes will follow the format (word_one, word_two): (word_three)\n",
    "            self.prefixes[prefix].append(suffix)\n",
    "            \n",
    "            # suffixes will follow the format (word_one, word_two): { word_three: count }\n",
    "            self.suffix_counts[prefix][suffix] += 1\n",
    "        \n",
    "        # the last two words of the text\n",
    "        end_prefix = tuple(words[-self.n:])\n",
    "        \n",
    "        # add boundaries after the last two words\n",
    "        # e.g. (second_last_word, last_word): [\"NONWORD\"]\n",
    "        self.prefixes[end_prefix].append(self.boundary)\n",
    "        \n",
    "        # similarly, add the ending sequence: (second_last_word, last_word): { \"NONWORD\": 1 }\n",
    "        self.suffix_counts[end_prefix][self.boundary] += 1\n",
    "    \n",
    "    def generate_text(self, length):\n",
    "        if not self.prefixes:\n",
    "            return \"\"\n",
    "        \n",
    "        # current_prefix: (\"NONWORD\", \"NONWORD\")\n",
    "        current_prefix = tuple([self.boundary] * self.n)\n",
    "        output = []\n",
    "        \n",
    "        # generate the text sequence for given length\n",
    "        while length > 0:\n",
    "            # this will start from the beginning\n",
    "            suffixes = self.prefixes[current_prefix]\n",
    "            \n",
    "            if not suffixes:\n",
    "                break\n",
    "            \n",
    "            # weights contain the count of suffixes\n",
    "            weights = [self.suffix_counts[current_prefix][s] for s in suffixes]\n",
    "            \n",
    "            # Select the next word from the list of suffixes, using the corresponding weights (frequencies).\n",
    "            next_word = random.choices(suffixes, weights=weights, k=1)[0]\n",
    "            \n",
    "            # if the next word is a boundary marker, then exit the loop\n",
    "            if next_word == self.boundary:\n",
    "                break\n",
    "            \n",
    "            output.append(next_word)\n",
    "            \n",
    "            # form the next prefix by taking the word_two from the current prefix and add the next word to it\n",
    "            # recall that a prefix is as follows: (word_one, word_two)\n",
    "            current_prefix = tuple(list(current_prefix[1:]) + [next_word])\n",
    "            \n",
    "            length -= 1\n",
    "        \n",
    "        return \" \".join(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9aae9169",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    cleaned_text = []\n",
    "    \n",
    "    for line in text:\n",
    "        tokens = word_tokenize(line.translate(str.maketrans(\n",
    "            \"\",\n",
    "            \"\",\n",
    "            string.punctuation\n",
    "        )))\n",
    "        cleaned_text.append(tokens)\n",
    "    \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b38bf348",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = []\n",
    "\n",
    "with open(\"./Harry_Potter_all_char_separated.txt\", encoding=\"utf-8\") as file:\n",
    "    lines = file.read().split(\"|\")\n",
    "    \n",
    "    for line in lines:\n",
    "        text.append(line.rstrip().lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b46b5485",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79731"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "71b6dd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_text = clean_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d1bc8b5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79731"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "206218f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gram = 4\n",
    "markov_chain = MarkovChain(n_gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "018b7082",
   "metadata": {},
   "outputs": [],
   "source": [
    "markov_chain.build_model(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1e5936d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chapter the boy who lived ” chapter the vanashig glass nearly ten years had passed since the dursleys had woken up to find their nephew on the front step but privet drive had hardly changed at all the sun rose on the same tidy front gardens and lit up the brass number four on the dursleys ’ shag carpet and covered in grimy rags aunt petunia let out a hair raising shriek nothing this filthy had entered her house in living memory dudley drew his large bare pink feet off the floor and was pointing at lupin wild eyed “ you you ” “ i ’ m going to have to start again from scratch now ” “ it was only a bit of mud to you boy but to me it ’ s blocked ” said harry shaking his head “ it ’ s not my fault ” said wood “ this is the best house i ’ ve ever seen ” “ fair point little bro ” said fred scanning their faces as they passed more cottages any one of them might have been the one in which james and lily had once lived or where bathilda lived\n"
     ]
    }
   ],
   "source": [
    "print(markov_chain.generate_text(200))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35373c72",
   "metadata": {},
   "source": [
    "## Modifiction\n",
    "`generate_text` is slightly modified to start from a user given phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "298ebc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarkovChain:\n",
    "    def __init__(self, n):\n",
    "        self.n = n\n",
    "        self.prefixes = defaultdict(list)\n",
    "        self.suffix_counts = defaultdict(lambda: defaultdict(int))\n",
    "        self.boundary = \"NONWORD\"\n",
    "    \n",
    "    def build_model(self, text):\n",
    "        words = [word for line in text for word in line]\n",
    "        words = [self.boundary] * self.n + words + [self.boundary]\n",
    "        \n",
    "        for i in range(len(words) - self.n):\n",
    "            prefix = tuple(words[i:i + self.n])\n",
    "            suffix = words[i + self.n]\n",
    "            self.prefixes[prefix].append(suffix)\n",
    "            self.suffix_counts[prefix][suffix] += 1\n",
    "        \n",
    "        end_prefix = tuple(words[-self.n:])\n",
    "        self.prefixes[end_prefix].append(self.boundary)\n",
    "        self.suffix_counts[end_prefix][self.boundary] += 1\n",
    "    \n",
    "    def generate_text(self, length, initial_phrase=None):\n",
    "        if not self.prefixes:\n",
    "            return \"\"\n",
    "        \n",
    "        if initial_phrase:\n",
    "            initial_words = initial_phrase.split()\n",
    "            \n",
    "            # inital phrase must be equal to n_gram\n",
    "            if len(initial_words) != self.n:\n",
    "                raise ValueError(\"Initial phrase should be == n_gram\")\n",
    "            \n",
    "            current_prefix = tuple(initial_words)\n",
    "        else:\n",
    "            current_prefix = tuple([self.boundary] * self.n)\n",
    "        \n",
    "        output = []\n",
    "        \n",
    "        while length > 0:\n",
    "            suffixes = self.prefixes[current_prefix]\n",
    "            \n",
    "            if not suffixes:\n",
    "                break\n",
    "            \n",
    "            weights = [self.suffix_counts[current_prefix][s] for s in suffixes]\n",
    "            next_word = random.choices(suffixes, weights=weights, k=1)[0]\n",
    "            \n",
    "            if next_word == self.boundary:\n",
    "                break\n",
    "            \n",
    "            output.append(next_word)\n",
    "            current_prefix = tuple(list(current_prefix[1:]) + [next_word])\n",
    "            length -= 1\n",
    "        \n",
    "        return initial_phrase + \" \" + \" \".join(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "48160675",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gram = 3\n",
    "markov_chain = MarkovChain(n_gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9ae19ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "markov_chain.build_model(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "cdca7d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bald head gleaming in the sunlight as the golden buttons on his waistcoat “ we are going to be able to find him i believed him finished i am not proud ” he whispered “ it ’ s a good one there had been nothing to how harry felt but they at least were sorry for dumbledore ’ s snitch and shook it hoping for he slipped his hand inside his jacket pocket harry moved toward him stretching out her short be ringed fingers for his arm and together they turned on the spot to disapparate as he turned to look at the size of a dog he was trying to get through a door left alone in london expecting the baby who would one day but your journey was pointless i never had the slightest difficulty in disbelieving snape ’ s desk “ am i to understand ” said harry “ and try to worm a confession out of him his scar still prickling his head threatening to split again dumbledore had warned us tha ’ migh ’ happen karkus knew enough to yell fer a couple o ’ swings at me when he finds i have done that he would not\n"
     ]
    }
   ],
   "source": [
    "print(markov_chain.generate_text(200, \"bald head gleaming\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5104484",
   "metadata": {},
   "source": [
    "## 5. Markov chain implementation using Markovify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd5e15b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import markovify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6893787",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = []\n",
    "\n",
    "with open(\"./Harry_Potter_all_char_separated.txt\", encoding=\"utf-8\") as file:\n",
    "    lines = file.read().split(\"|\")\n",
    "    \n",
    "    for line in lines:\n",
    "        text.append(line.rstrip().lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f6af6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_model = markovify.Text(text, state_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cf0cb809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "snape advanced on ron slowly , and as he was that stood on a hill overlooking the village , the wind roared , but still he did not see them exchange a word all the time .\n"
     ]
    }
   ],
   "source": [
    "print(text_model.make_sentence_with_start(\"snape advanced on\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656cf055",
   "metadata": {},
   "source": [
    "## The end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
