{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28f27358",
   "metadata": {},
   "source": [
    "# Goals of this notebook\n",
    "- Deep Learning overview\n",
    "- Keras Basics\n",
    "- Basics of LSTM and RNN\n",
    "- Text generation from text corpus using LSTM\n",
    "- Q/A Chat bots using End-to-End RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda61169",
   "metadata": {},
   "source": [
    "## 1. Perceptron Model\n",
    "Artificial Neural Networks (ANN) are inspired by the human brain's biological neural network, specifically the [neuron](https://en.wikipedia.org/wiki/Neuron). A neuron in the human brain receives input from various dendrites, processes it in the cell body, and transmits output through the axon to other neurons. This process is mirrored in ANNs through the perceptron model.\n",
    "\n",
    "**Biological Neuron**<br>\n",
    "A biological neuron consists of three main parts:\n",
    "- **Dendrites**: these receive signals from other neurons.\n",
    "- **Cell Body (Soma)**: This processes the incoming signals.\n",
    "- **Axon**: This transmits the processed signals to other neurons.\n",
    "\n",
    "### 1.1 Artificial Neuron (Perceptron)\n",
    "An artificial neuron, or perceptron, mimics this biological structure. It consists of:\n",
    "\n",
    "- **Input**: Similar to dendrites, it receives multiple inputs.\n",
    "- **Weights**: Each input is assigned a weight which determines its significance.\n",
    "- **Summation Function**: It sums the weighted inputs.\n",
    "- **Activation Function**: It processes the summed input to produce an output.\n",
    "- **Output**: Similar to the axon, it transmits the signal to the next layer of neurons.\n",
    "\n",
    "### 1.2 Mathematical Representation\n",
    "In mathematical terms, the perceptron model can be described using the following equations:\n",
    "\n",
    "#### 1.2.1 Weighted Sum:\n",
    "\n",
    "$$ z = \\sum_{i=0}^n w_i x_i + b $$\n",
    "\n",
    "Where:\n",
    "- $ x_i $ are the input features.\n",
    "- $ w_i $ are the weights corresponding to each input.\n",
    "- $ b $ is the bias term.\n",
    "- $ z $ is the weighted sum.\n",
    "\n",
    "#### 1.2.2 Activation Function\n",
    "$$ y = f(x) $$\n",
    "\n",
    "Where $ f $ is the activation function. Common activation functions include the step function, sigmoid function, and ReLU (Rectified Linear Unit).\n",
    "\n",
    "For example, the step activation function can be defined as:\n",
    "\n",
    "$$ f(z) = \n",
    "\\begin{cases} \n",
    "1 & \\text{if } z \\geq 0 \\\\\n",
    "0 & \\text{if } z < 0 \n",
    "\\end{cases} $$\n",
    "\n",
    "#### 1.2.3 Sigmoid Function\n",
    "The sigmoid function is defined as:\n",
    "\n",
    "$$ f(z) = \\frac{1}{1 + e^{-z}} $$\n",
    "\n",
    "#### 1.2.4 ReLU (Rectified Linear Unit)\n",
    "The ReLU activation function is defined as:\n",
    "\n",
    "$$ f(z) = \\max(0, z) $$\n",
    "\n",
    "### 1.3 Perceptron Algorithm\n",
    "The perceptron algorithm updates the weights based on the error in prediction. The steps are as follows:\n",
    "\n",
    "- Set initial weights to small random numbers.\n",
    "- Compute the weighted sum $ z $ which is summation of all inputs multiplied with the respective weights added with the bias term.\n",
    "- Apply the activation function to get the predicted output $ y $.\n",
    "- Update the weights based on the error $ (y_\\text{true} - y_\\text{pred}) $\n",
    "<br>\n",
    "\n",
    "$$ w_i = w_i + \\Delta w_i $$\n",
    "<br>\n",
    "\n",
    "<center>Where:</center>\n",
    "\n",
    "$$ \\Delta w_i = \\eta (y_{true} - y_{pred}) x_i $$\n",
    "<br>\n",
    "\n",
    "<center>and $ \\eta $ is the learning rate.</center>\n",
    "\n",
    "- Repeat the process for a fixed number of iterations or until the error is minimized.\n",
    "\n",
    "### 1.4 Visual Representation\n",
    "![Perceptron](https://gamedevacademy.org/wp-content/uploads/2017/09/Single-Perceptron.png.webp)\n",
    "\n",
    "## 2. Neural Network\n",
    "A neural network consists of interconnected layers of nodes, or \"neurons,\" each layer performing specific transformations on the input data. The basic structure of a neural network includes:\n",
    "\n",
    "- **Input Layer**: The layer that receives the input data.\n",
    "- **Hidden Layer**: Itermediate layers that transform the input data through learned weights and activation functions.\n",
    "- **Output Layer**: The layer that produces the final output of the network.\n",
    "\n",
    "### 2.1 Weighted Sum (or Linear Combination)\n",
    "Each neuron computes a weighted sum of its inputs plus a bias term. For a neuron $ j $ in layer $ l $, this can be expressed as:\n",
    "\n",
    "$$ z_j^{(l)} = \\sum_{i} w_{ij}^{(l)} a_i^{(l-1)} + b_j^{(l)} $$\n",
    "\n",
    "Where:\n",
    "- $ w_{ij}^{(l)} $ is the weight from neuron $ i $ in layer $ L - 1 $ to neuron $ j $ in layer $ l $.\n",
    "- $ a_i^{(l-1)} $ is the activation of neuron $ i $ in layer $ L - 1 $.\n",
    "- $ b_j^{(l)} $ is the bias term of neuron $ j $ in layer $ L $.\n",
    "\n",
    "### 2.2 Activation Function\n",
    "The activation function $ \\sigma $ is applied to the weighted sum to produce the neuron's output:\n",
    "\n",
    "$$ a_{j}^{(l)} = \\sigma{(z_{j}^{(l)})} $$\n",
    "\n",
    "### 2.3 Loss Function\n",
    "The loss function quantifies the difference between the predicted output and the true output. A common loss function for classification is the cross-entropy loss:\n",
    "\n",
    "$$ L(y, \\hat{y}) = - \\sum_{k} y_k \\log(\\hat{y}_k) $$\n",
    "\n",
    "Where:\n",
    "- $ y_k $ is the true label for class $ k $.\n",
    "- $ \\hat{y_k} $ is the predicted probability for class $ k $.\n",
    "\n",
    "For regression, a common loss function is the Mean Squared Error (MSE).\n",
    "\n",
    "$$ L(y, \\hat{y}) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 $$\n",
    "\n",
    "Where:\n",
    "- $ y_i $ is the true value.\n",
    "- $ \\hat{y_i} $ is the predicted value.\n",
    "- $ n $ is the number of samples.\n",
    "\n",
    "### 2.4 Backpropagation and Gradient Descent\n",
    "Backpropagation is used to update the weights and biases by computing gradients of the loss function with respect to each weight and bias. The weights are updated using gradient descent:\n",
    "\n",
    "$$ w_{ij}^{(l)} \\leftarrow w_{ij}^{(l)} - \\eta \\frac{\\partial L}{\\partial w_{ij}^{(l)}} $$\n",
    "\n",
    "Where:\n",
    "- $ \\eta $ is the learning rate.\n",
    "- $ \\frac{\\partial L}{\\partial w_{ij}^{(l)}} $ is the gradient of the loss with respect to the weight $ w_{ij}^{(l)} $.\n",
    "\n",
    "### 2.5 Update Rule for Biases\n",
    "Biases are updated using;\n",
    "\n",
    "$$ b_j^{(l)} \\leftarrow b_j^{(l)} - \\eta \\frac{\\partial L}{\\partial b_j^{(l)}} $$\n",
    "\n",
    "Where:\n",
    "- $ \\frac{\\partial L}{\\partial b_j^{(l)}} $ is the gradient of the loss with respect to the bias $ b_j^{(l)} $\n",
    "\n",
    "For further in-depth reading please follow these resources:\n",
    "- [Backpropogation Step by Step](https://hmkcode.com/ai/backpropagation-step-by-step/)\n",
    "- [A Step by Step Backpropogation example](https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b7679f",
   "metadata": {},
   "source": [
    "## 3. Recurrent Neural Network\n",
    "A Recurrent Neural Network (RNN) is a type of neural network designed for processing sequences of data. Unlike traditional feedforward neural networks, RNNs have connections that form directed cycles, allowing them to maintain a state or memory of previous inputs. This capability makes them suitable for tasks involving sequential data, such as time series forecasting, natural language processing.\n",
    "\n",
    "Recall, that a normal neuron in a feed forward neural network takes in an input (multiple inputs are aggregated) and passes it through some sort of activation function and produces a output.\n",
    "\n",
    "![Normal Neuron](https://www.dropbox.com/scl/fi/5tarivnndy1q41v7g1owq/Normal_Neuron.png?rlkey=sh8or3pso3pn15it0fte4dm3o&st=dea6oax9&raw=1)\n",
    "\n",
    "A recurrent neuron can send output back to itself.\n",
    "\n",
    "![Recurrent Neuron](https://www.dropbox.com/scl/fi/deykm5b9dmw17kf09yq2p/Recurrent-Neuron.png?rlkey=1u3vrbqsduwm73vo6ypbnurxd&st=ya8sfmdv&raw=1)\n",
    "\n",
    "We can then unroll this throughout time.\n",
    "\n",
    "![Recurrent Neuron Unroll](https://www.dropbox.com/scl/fi/9w6amld3t3goehszzt0ps/Recurrent_Neurons_Unroll.png?rlkey=x4i9l7dzhqlt4gmta7jdxvza8&st=gzdm77ot&raw=1)\n",
    "\n",
    "Cells that are a function of inputs from previous time stteps are also known as **memory cells**.\n",
    "\n",
    "The cells can then be combined to form layers and the same concept of unrolling will be applied, where each layer produces an output and passes it back to the neurons.\n",
    "\n",
    "### 3.1 LSTM (Long Short-Term Memory)\n",
    "An issue RNNs face is that after a while the network starts to \"forget\" the first inputs, as the information is lost at each step going through the RNN. To mitigate that, some sort of \"long-term memory\" is required for the networks.\n",
    "\n",
    "LSTM cell was created to help address these RNN issues. Visually it is structured as shown in the below image:\n",
    "\n",
    "![LSTM Cell](https://www.dropbox.com/scl/fi/2qvarm242pkrq08fkeh63/lstm-3.svg?rlkey=4jouh3i5kqdyztnvr50h68xck&st=oxhxhzr3&raw=1)\n",
    "\n",
    "#### The very first step is called the **forget gate layer**. In this step, we decide what information are we going to forget or throw away from the cell state.\n",
    "\n",
    "$$ \\mathbf{f}_t = \\sigma(\\mathbf{W}_f \\cdot [h_{t - 1}, x_t] + b_f) $$\n",
    "\n",
    "Where:\n",
    "- $ f_t $: The forget gate's output at time $ t $. It is a vector of values between 0 and 1, representing how much of each element in the previous cell state should be forgotten.\n",
    "<br>\n",
    "\n",
    "- $ \\sigma $: The sigmoid activation function, which maps its input to a value between 0 and 1. This ensures that the forget gate's output is in the range [0, 1], where 0 means \"completely forget\" and 1 means \"completely retain\".\n",
    "<br>\n",
    "\n",
    "- $ W_f $: The weight matrix associated with the forget gate. This matrix is learned during training and is used to transform the combined input.\n",
    "<br>\n",
    "\n",
    "- $ [h_{t - 1}, x_t] $ The concatenation of the previous hidden state $ h_{t - 1} $ and the current input $ x_t $. The concatenation allows the forget gate to use information from both the previous step and the current input to decide what to forget.\n",
    "<br>\n",
    "\n",
    "- $ b_f $: The bias term for the forget gate, which is added to the weighted sum before applying the activation function.\n",
    "<br>\n",
    "\n",
    "#### The next step is to decide what new information are we going to store in the cell state. For that we have two layers.\n",
    "\n",
    "The first layer is called the **input gate layer** or sigmoid layer. It controls how much of the new information will be added to the cell state.\n",
    "\n",
    "$$ \\underset{\\text{The sigmoid layer}}{\\mathbf{i}_t = \\sigma(\\mathbf{W}_i \\cdot [h_{t - 1}, x_t] + b_i)} $$\n",
    "\n",
    "Where:\n",
    "- $\\mathbf{i}_t$ is the output of the input gate at time $t$. It represents how much of the new information should be added to the cell state. It is a vector with values between 0 and 1.\n",
    "<br>\n",
    "\n",
    "- $\\sigma$ denotes the sigmoid activation function, which squashes the input to a value between 0 and 1.\n",
    "<br>\n",
    "\n",
    "- $\\mathbf{W}_i$ is the weight matrix for the input gate. It transforms the concatenated vector $[h_{t - 1}, x_t]$ into a space where the sigmoid function can be applied.\n",
    "<br>\n",
    "\n",
    "- $[h_{t - 1}, x_t]$ is the concatenation of the previous hidden state $h_{t - 1}$ and the current input $x_t$.\n",
    "<br>\n",
    "\n",
    "- $b_i$ is the bias term for the input gate, added to the weighted sum before applying the sigmoid activation function.\n",
    "\n",
    "$$ \\underset{\\text{The hyperbolic tangent layer}}{\\tilde{C}_t = \\tanh(\\mathbf{W}_C \\cdot [h_{t - 1}, x_t] + b_C)} $$\n",
    "\n",
    "Where:\n",
    "- $\\tilde{C}_t$ is the output of the hyperbolic tangent layer at time $t$. It represents the candidate values that can be added to the cell state. The values are in the range $[-1, 1]$.\n",
    "<br>\n",
    "- $\\tanh$ denotes the hyperbolic tangent activation function, which maps the input to the range $[-1, 1]$.\n",
    "<br>\n",
    "\n",
    "- $\\mathbf{W}_C$ is the weight matrix for the hyperbolic tangent layer. It transforms the concatenated vector $[h_{t - 1}, x_t]$ into a space where the $\\tanh$ function can be applied.\n",
    "<br>\n",
    "\n",
    "- $[h_{t - 1}, x_t]$ is the concatenation of the previous hidden state $h_{t - 1}$ and the current input $x_t$.\n",
    "<br>\n",
    "\n",
    "- $b_C$ is the bias term for the hyperbolic tangent layer, added to the weighted sum before applying the $\\tanh$ activation function.\n",
    "\n",
    "#### The next step is to update the old cell state ($ C_{t - 1} $) to $ C_t $\n",
    "$$ C_t = \\mathbf{f}_t \\cdot C_{t - 1} + \\mathbf{i}_t \\cdot \\tilde{C}_t $$\n",
    "\n",
    "Where:\n",
    "- $C_t$ is the updated cell state at time $t$.\n",
    "<br>\n",
    "- $\\mathbf{f}_t$ is the forget gate output, determining how much of the previous cell state $C_{t - 1}$ should be retained.\n",
    "<br>\n",
    "\n",
    "- $C_{t - 1}$ is the cell state from the previous time step.\n",
    "<br>\n",
    "\n",
    "- $\\mathbf{i}_t$ is the input gate output, controlling how much of the new candidate values $\\tilde{C}_t$ should be added to the cell state.\n",
    "<br>\n",
    "\n",
    "- $\\tilde{C}_t$ is the candidate values from the hyperbolic tangent layer.\n",
    "\n",
    "#### The final decision is what to output for $ h_t $\n",
    "The output gate determines how much of the cell state should be exposed to the hidden state. The formula for the output gate is:\n",
    "\n",
    "$$ \\mathbf{o}_t = \\sigma(\\mathbf{W}_o \\cdot [h_{t - 1}, x_t] + b_o) $$\n",
    "\n",
    "Where:\n",
    "- $\\mathbf{o}_t$ is the output gate at time $t$. It controls how much of the updated cell state $C_t$ should be exposed as the hidden state $h_t$.\n",
    "<br>\n",
    "\n",
    "- $\\sigma$ denotes the sigmoid activation function, which squashes the input to a value between 0 and 1.\n",
    "<br>\n",
    "\n",
    "- $\\mathbf{W}_o$ is the weight matrix for the output gate. It transforms the concatenated vector $[h_{t - 1}, x_t]$ into a space where the sigmoid function can be applied.\n",
    "<br>\n",
    "\n",
    "- $[h_{t - 1}, x_t]$ is the concatenation of the previous hidden state $h_{t - 1}$ and the current input $x_t$.\n",
    "- $b_o$ is the bias term for the output gate, added to the weighted sum before applying the sigmoid activation function.\n",
    "\n",
    "The hidden state is updated as follows:\n",
    "\n",
    "$$ h_t = \\mathbf{o}_t \\cdot \\tanh(C_t) $$\n",
    "\n",
    "Where:\n",
    "- $h_t$ is the new hidden state at time $t$.\n",
    "<br>\n",
    "\n",
    "- $\\mathbf{o}_t$ is the output gate, which determines how much of the updated cell state $C_t$ should be exposed in the hidden state.\n",
    "<br>\n",
    "\n",
    "- $\\tanh(C_t)$ applies the hyperbolic tangent function to the updated cell state $C_t$, normalizing it to the range $[-1, 1]$.\n",
    "\n",
    "### 3.2 GRU (Gate Recurrent Unit)\n",
    "Gated Recurrent Units (GRUs) were proposed in the paper “Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation” by Cho et al. (2014). The primary motivation behind creating GRUs was to simplify the LSTM architecture while retaining its capability to model long-term dependencies and handle vanishing gradient problems effectively.\n",
    "\n",
    "![GRU](https://www.dropbox.com/scl/fi/58g9p0qbtpnxkcrnxe0qv/gru-3.svg?rlkey=cgyrqage3v9vewu9p835xmm3f&st=n1fs7889&raw=1)\n",
    "\n",
    "GRU has a simplified architecture with two gates: the update gate (z) and reset gate (r).\n",
    "\n",
    "#### The update gate controls how much of the previous hidden state should be retained.\n",
    "\n",
    "$$ \\mathbf{z}_t = \\sigma(\\mathbf{W}_z \\cdot [h_{t - 1}, x_t] + b_z) $$\n",
    "\n",
    "Where:\n",
    "- $\\mathbf{z}_t$: Update gate at time $t$. It controls how much of the previous memory should be kept.\n",
    "<br>\n",
    "- $\\sigma$: Sigmoid activation function.\n",
    "<br>\n",
    "\n",
    "- $\\mathbf{W}_z$: Weight matrix for the update gate.\n",
    "<br>\n",
    "\n",
    "- $[h_{t - 1}, x_t]$: Concatenation of the previous hidden state $h_{t - 1}$ and the current input $x_t$.\n",
    "<br>\n",
    "\n",
    "- $b_z$: Bias term for the update gate.\n",
    "\n",
    "#### The reset gate determines how much of the past information to forget. The formula for reset gate is as follows.\n",
    "\n",
    "$$ \\mathbf{r}_t = \\sigma(\\mathbf{W}_r \\cdot [h_{t - 1}, x_t] + b_r) $$\n",
    "\n",
    "Where:\n",
    "- $\\mathbf{r}_t$: Reset gate at time $t$. It controls how much of the previous memory should be discarded.\n",
    "<br>\n",
    "\n",
    "- $\\sigma$: Sigmoid activation function.\n",
    "<br>\n",
    "\n",
    "- $\\mathbf{W}_r$: Weight matrix for the reset gate.\n",
    "<br>\n",
    "\n",
    "- $[h_{t - 1}, x_t]$: Concatenation of the previous hidden state $h_{t - 1}$ and the current input $x_t$.\n",
    "<br>\n",
    "\n",
    "- $b_r$: Bias term for the reset gate.\n",
    "\n",
    "#### The candidate hidden state is computed using the reset gate and the current input. It calculates the new memory content candidate based on the reset gate's influence on the previous hidden state.\n",
    "\n",
    "$$ \\tilde{h}_t = \\tanh \\left( \\mathbf{W}_h \\cdot [(\\mathbf{r}_t \\odot h_{t - 1}), x_t] + b_h \\right) $$\n",
    "\n",
    "Where:\n",
    "- $\\tilde{h}_t$ is the candidate hidden state at time $t$.\n",
    "<br>\n",
    "- $\\tanh$ is the hyperbolic tangent activation function.\n",
    "<br>\n",
    "\n",
    "- $\\mathbf{W}_h$ is the weight matrix for the candidate hidden state.\n",
    "<br>\n",
    "\n",
    "- $\\mathbf{r}_t$ is the reset gate at time $t$.\n",
    "<br>\n",
    "\n",
    "- $h_{t - 1}$ is the previous hidden state.\n",
    "<br>\n",
    "\n",
    "- $x_t$ is the current input at time $t$.\n",
    "<br>\n",
    "\n",
    "- $b_h$ is the bias term for the candidate hidden state.\n",
    "<br>\n",
    "\n",
    "- $\\odot$ denotes element-wise multiplication.\n",
    "\n",
    "#### The final memory (hidden state) at the current time step is computed as:\n",
    "\n",
    "$$ h_t = \\mathbf{z}_t \\odot h_{t - 1} + (1 - \\mathbf{z}_t) \\odot \\tilde{h}_t $$\n",
    "\n",
    "Where:\n",
    "- $h_t$: Final hidden state at time $t$.\n",
    "<br>\n",
    "\n",
    "- $\\mathbf{z}_t$: Update gate at time $t$.\n",
    "<br>\n",
    "\n",
    "- $h_{t - 1}$: Previous hidden state.\n",
    "<br>\n",
    "\n",
    "- $\\tilde{h}_t$: New memory content candidate.\n",
    "<br>\n",
    "\n",
    "- $\\odot$: Element-wise multiplication.\n",
    "<br>\n",
    "\n",
    "- $1 - \\mathbf{z}_t$: The complement of the update gate, used to weigh the new memory content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad88293f",
   "metadata": {},
   "source": [
    "## 4. Text Generation with Keras and RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "649c9e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25187f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we only need spacy for tokenization\n",
    "NLP = spacy.load(\"en_core_web_md\", disable=[\"parser\", \"tagger\", \"ner\", \"lemmatizer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42e943c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# otherwise spaCy will complain about text limit exceeded\n",
    "NLP.max_length = 1115394"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22dd2825",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_from_url(url):\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        content = response.text\n",
    "        return content\n",
    "    else:\n",
    "        return f\"Error: Unable to fetch content. Status code: {response.status_code}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "321e6b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_punc(text):\n",
    "    return [\n",
    "        token.text.lower() for token in NLP(text) if token.text not in \"\\n\\n \\n\\n\\n!\\\"-$#%&()--.*+,-/;:<=>?@[\\\\]^_`{|}~\\t\\n\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "080df233",
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_content = read_from_url(\"https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4428e663",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_chars = sorted(set(shakespeare_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eaa25099",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unique_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76678e1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n',\n",
       " ' ',\n",
       " '!',\n",
       " '$',\n",
       " '&',\n",
       " \"'\",\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '3',\n",
       " ':',\n",
       " ';',\n",
       " '?',\n",
       " 'A',\n",
       " 'B',\n",
       " 'C',\n",
       " 'D',\n",
       " 'E',\n",
       " 'F',\n",
       " 'G',\n",
       " 'H',\n",
       " 'I',\n",
       " 'J',\n",
       " 'K',\n",
       " 'L',\n",
       " 'M',\n",
       " 'N',\n",
       " 'O',\n",
       " 'P',\n",
       " 'Q',\n",
       " 'R',\n",
       " 'S',\n",
       " 'T',\n",
       " 'U',\n",
       " 'V',\n",
       " 'W',\n",
       " 'X',\n",
       " 'Y',\n",
       " 'Z',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3860ba2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = separate_punc(shakespeare_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa6c228c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "207859"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e31a3f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequence length is 50 i.e. the model will look at the 50 words\n",
    "# and predict the next word\n",
    "training_length = 50 + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125adc1f",
   "metadata": {},
   "source": [
    "The below code generates overlapping sequences from a list of tokens, where each sequence starts one token later than the previous sequence\n",
    "\n",
    "**Example**:\n",
    "`tokens = [\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\", \"and\", \"the\", \"cat\", \"purred\"]`\n",
    "`training_length = 2 + 1`\n",
    "\n",
    "**Sequences after running the code**:\n",
    "- `[\"The\", \"cat\", \"sat\"]`\n",
    "- `[\"cat\", \"sat\", \"on\"]`\n",
    "- `[\"sat\", \"on\", \"the\"]`\n",
    "- `[\"on\", \"the\", \"mat\"]`\n",
    "- `[\"the\", \"mat\", \"and\"]`\n",
    "- `[\"mat\", \"and\", \"the\"]`\n",
    "- `[\"and\", \"the\", \"cat\"]`\n",
    "- `[\"the\", \"cat\", \"purred\"]`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d61e651b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_sequences = []\n",
    "\n",
    "for idx in range(training_length, len(tokens)):\n",
    "    sequence = tokens[idx - training_length:idx]\n",
    "    text_sequences.append(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1bc27dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "61e92456",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "10b71258",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(text_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b76e5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(text_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeee778f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary created by the tokenizer\n",
    "tokenizer.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3c83005c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[93, 277, 142, 33, 991, 147, 673, 133, 17, 112]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the first 10 tokenized words\n",
    "sequences[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0866dedb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['first', 'citizen', 'before', 'we', 'proceed', 'any', 'further', 'hear', 'me', 'speak']\n"
     ]
    }
   ],
   "source": [
    "print([tokenizer.index_word[i] for i in sequences[0][:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69da8b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer also creates a word count dictionary\n",
    "tokenizer.word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ee43f142",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7d953ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = np.array(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c77ff613",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   93,   277,   142, ...,    64,    83,   498],\n",
       "       [  277,   142,    33, ...,    83,   498,    28],\n",
       "       [  142,    33,   991, ...,   498,    28,     2],\n",
       "       ...,\n",
       "       [ 2011,    16,    12, ...,   357, 12327,  1084],\n",
       "       [   16,    12,     8, ..., 12327,  1084,    27],\n",
       "       [   12,     8,  3871, ...,  1084,    27,   134]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ea7c2222",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "88f1b2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for every row, grab all the columns except the last one\n",
    "X = sequences[:, :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "185e1a1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   93,   277,   142, ...,   277,    64,    83],\n",
       "       [  277,   142,    33, ...,    64,    83,   498],\n",
       "       [  142,    33,   991, ...,    83,   498,    28],\n",
       "       ...,\n",
       "       [ 2011,    16,    12, ...,   208,   357, 12327],\n",
       "       [   16,    12,     8, ...,   357, 12327,  1084],\n",
       "       [   12,     8,  3871, ..., 12327,  1084,    27]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences[:, :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a8c473fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for every row, grab the last column\n",
    "y = sequences[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2465f467",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 498,   28,    2, ..., 1084,   27,  134])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "55668aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras needs padded number of classes to hold an extra zero at the end\n",
    "y = to_categorical(y, num_classes=len(tokenizer.word_counts) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1bc86262",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(207808, 50)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# note that 50 is the sequence length\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "36960118",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7eda9740",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_counts) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fdf77e",
   "metadata": {},
   "source": [
    "### Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6f5c743f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, LSTM, Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c74af5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Input(shape=(sequence_length,)))\n",
    "model.add(Embedding(vocab_size, sequence_length))\n",
    "model.add(LSTM(sequence_length, return_sequences=True))\n",
    "model.add(LSTM(sequence_length))\n",
    "model.add(Dense(sequence_length * 2, activation=\"relu\"))\n",
    "\n",
    "model.add(Dense(vocab_size, activation=\"softmax\"))\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4fe15153",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">616,400</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)              │          <span style=\"color: #00af00; text-decoration-color: #00af00\">20,200</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)                  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">20,200</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)                 │           <span style=\"color: #00af00; text-decoration-color: #00af00\">5,100</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12328</span>)               │       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,245,128</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m50\u001b[0m)              │         \u001b[38;5;34m616,400\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m50\u001b[0m)              │          \u001b[38;5;34m20,200\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)                  │          \u001b[38;5;34m20,200\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)                 │           \u001b[38;5;34m5,100\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12328\u001b[0m)               │       \u001b[38;5;34m1,245,128\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,907,028</span> (7.27 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,907,028\u001b[0m (7.27 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,907,028</span> (7.27 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,907,028\u001b[0m (7.27 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3ec422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't have enough memory on my local PC (training may take a very long time)\n",
    "history = model.fit(X, y, batch_size=10, epochs=20, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a57184",
   "metadata": {},
   "source": [
    "### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa74b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ad32db",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"shakespeare_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31b4981",
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(tokenizer, open(\"shakespeare_tokenizer\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd2e207",
   "metadata": {},
   "source": [
    "### Generate new text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ef9ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cf11dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_txt = []\n",
    "input_txt = \"Than of your graces and your gifts to tell; Which borrowed from this holy fire of Love That they elsewhere might dart their\"\n",
    "\n",
    "# generate 20 words\n",
    "for idx in range(20):\n",
    "    encoded = tokenizer.texts_to_sequences([input_txt])[0]\n",
    "    \n",
    "    # if the initial text is too long or too short (pad it)\n",
    "    encoded_padded = pad_sequences(\n",
    "        [encoded],\n",
    "        maxlen=sequence_length,\n",
    "        truncating=\"pre\"\n",
    "    )\n",
    "    \n",
    "    pred_word = model.predict(encoded_padded, verbose=0)[0]\n",
    "    pred_word_idx = np.argmax(pred_word, axis=1)\n",
    "    pred_word = tokenizer.index_word[pred_word_idx]\n",
    "    \n",
    "    input_txt += \" \" + pred_word\n",
    "    \n",
    "    output_txt.append(pred_word)\n",
    "    \n",
    "print(\" \".join(output_txt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3eeeed0",
   "metadata": {},
   "source": [
    "### The End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
