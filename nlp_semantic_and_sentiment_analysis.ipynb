{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c051838",
   "metadata": {},
   "source": [
    "# Goals of this notebook\n",
    "- Semantic Vectors\n",
    "- Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b03f17",
   "metadata": {},
   "source": [
    "## 1. Word Vectors\n",
    "Word vectors, also known as word embeddings, are numerical vector representations of words that capture semantic meaning by placing words with similar meanings close to each other in a multi-dimensional space. These vectors are used extensively in natural language processing (NLP) tasks such as word similarity, sentiment analysis, and machine translation.\n",
    "\n",
    "Word embeddings are typically learned from large text corpora using various methods. Some popular algorithms for generating word vectors include Word2Vec, GloVe (Global Vectors for Word Representation), and FastText.\n",
    "\n",
    "### 1.1 Word2Vec\n",
    "Word2Vec is a neural network-based approach to learning distributed representations of words, i.e., word embeddings. It was introduced by Mikolov et al. in 2013. The main idea is to learn a vector representation for each word such that words with similar meanings are close to each other in the vector space. It comes in two main variants: **Continuous Bag of Words (CBOW)** and **Skip-gram**.\n",
    "\n",
    "### 1.2 Skip-gram Model\n",
    "In the Skip-gram model, the objective is to predict the context words given a target word. The model maximizes the probability of the context words given the target word over a large corpus of text.\n",
    "\n",
    "The objective function for the Skip-gram model is:\n",
    "\n",
    "$$ \\max \\frac{1}{T} \\sum_{t=1}^{T} \\sum_{-c \\leq j \\leq c, j \\neq 0} \\log P(w_{t+j} \\mid w_t) $$\n",
    "\n",
    "Where:\n",
    "- $ T $ is the total number of words in the corpus\n",
    "- $ c $ is the context window size\n",
    "- $ w_t $ is the target word at position $ t $\n",
    "- $ w_t+j $ are the context words within the window size $ c $ around the target word.\n",
    "\n",
    "The probability $ P(w_O \\mid w_I) $ is usually computed using the softmax function.\n",
    "\n",
    "$$ P(w_O \\mid w_I) = \\frac{\\exp(\\mathbf{v}_{w_O} \\cdot \\mathbf{v}_{w_I})}{\\sum_{w \\in V} \\exp(\\mathbf{v}_w \\cdot \\mathbf{v}_{w_I})} $$\n",
    "\n",
    "Where:\n",
    "- $ v_{w_0} $ is the vector representation of the context word $ w_O $.\n",
    "- $ v_{w_1} $ is the vector representation of the input (target) word $ w_I $.\n",
    "- $ V $ is the vocabulary size.\n",
    "\n",
    "### 1.3 Continuous Bag of Words (CBOW) Model\n",
    "The CBOW model is the inverse of the Skip-gram model. It predicts the target word given the context words. The objective is to maximize the probability of the target word given the context words.\n",
    "\n",
    "The objective function for the CBOW model is:\n",
    "\n",
    "$$ \\max \\frac{1}{T} \\sum_{t=1}^{T} \\log P(w_t \\mid w_{t-c}, \\ldots, w_{t-1}, w_{t+1}, \\ldots, w_{t+c}) $$\n",
    "\n",
    "The probability is computed using the softmax function in a similar manner to the Skip-gram model.\n",
    "\n",
    "### 1.4 Negative Sampling\n",
    "To make the training more efficient, Word2Vec uses a technique called negative sampling. Instead of computing the softmax over the entire vocabulary, it samples a few negative examples (words that are not in the context) for each positive example (word in the context).\n",
    "\n",
    "The objective function with negative sampling is:\n",
    "\n",
    "$$ \\log \\sigma(\\mathbf{v}_{w_O} \\cdot \\mathbf{v}_{w_I}) + \\sum_{i=1}^{k} \\mathbb{E}_{w_i \\sim P_n(w)} \\left[ \\log \\sigma(-\\mathbf{v}_{w_i} \\cdot \\mathbf{v}_{w_I}) \\right] $$\n",
    "\n",
    "Where:\n",
    "- $ \\sigma $ is the sigmoid function\n",
    "- $ k $ is the number of negative samples\n",
    "- $ P_n(w) $ is the noise distribution of negative sampling $\n",
    "\n",
    "### 1.5 Visual Representation of CBOW and Skip-gram\n",
    "![CBOW & Skip-gram](https://www.researchgate.net/profile/Wang-Ling-16/publication/281812760/figure/fig1/AS:613966665486361@1523392468791/Illustration-of-the-Skip-gram-and-Continuous-Bag-of-Word-CBOW-models.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d28ded",
   "metadata": {},
   "source": [
    "## 2. Cosine Similarity\n",
    "Cosine similarity is a metric used to measure how similar two vectors are in terms of their orientation, irrespective of their magnitude. It’s commonly used in text analysis, particularly with word embeddings, to compare the similarity between two words or documents.\n",
    "\n",
    "Cosine similarity between vectors \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) is:\n",
    "\n",
    "$$ \\text{Cosine Similarity}(\\mathbf{A}, \\mathbf{B}) = \\frac{\\mathbf{A} \\cdot \\mathbf{B}}{\\|\\mathbf{A}\\| \\|\\mathbf{B}\\|} $$\n",
    "\n",
    "Where:\n",
    "- $ A \\cdot B $ is the dot product of vectors $ A $ and $ B $.\n",
    "- $ \\|A\\| \\text{and} \\|B\\| $ are the magnitudes or norms of vectors $ A $ and $ B $, respectively.\n",
    "\n",
    "Cosine similarity ranges from -1 to 1:\n",
    "- $ 1 $ indicates that the vectors are identical (i.e., pointing in the same direction).\n",
    "- $ 0 $ indicates orthogonality (i.e., no similarity).\n",
    "- $ -1 $ indicates that the vectors are diametrically opposed (i.e., pointing in opposite directions).\n",
    "\n",
    "### 2.1 Dot Product\n",
    "The dot product of two vectors measures their similarity by calculating the sum of the products of their corresponding components. The formula for the dot product of vectors $ A $ and $ B $ is:\n",
    "\n",
    "$$ \\mathbf{A} \\cdot \\mathbf{B} = \\sum_{i=1}^{n} A_i B_i $$\n",
    "\n",
    "**Example**:\n",
    "\n",
    "For vectors $ A = [1, 2, 3] and B = [4, 5, 6] $\n",
    "\n",
    "$$ A \\cdot B = (1 \\cdot 4) + (2 \\cdot 5) + (3 \\cdot 6) = 4 + 10 + 18 = 32 $$\n",
    "\n",
    "### 2.2 Magnitude (Norm)\n",
    "The magnitude (or norm) of a vector is a measure of its length. For a vector $ A $, the magnitude is calculated as:\n",
    "\n",
    "$$ \\|\\mathbf{A}\\| = \\sqrt{\\sum_{i=1}^{n} A_i^2} $$\n",
    "\n",
    "**Example**:\n",
    "\n",
    "For vector $ A = [1, 2, 3] $\n",
    "\n",
    "$$ \\|\\mathbf{A}\\| = \\sqrt{1^2 + 2^2 + 3^2} = \\sqrt{1 + 4 + 9} = \\sqrt{14} \\approx 3.74 $$\n",
    "\n",
    "For vector $ B = [4, 5, 6] $\n",
    "\n",
    "$$ \\|\\mathbf{B}\\| = \\sqrt{4^2 + 5^2 + 6^2} = \\sqrt{16 + 25 + 36} = \\sqrt{77} \\approx 8.77 $$\n",
    "\n",
    "### 2.3 Cosine Similarity (Calculation)\n",
    "Once we have calculated the dot product and magnitudes, we can then plug the values in the formula and get the answer.\n",
    "\n",
    "$$ \\text{Cosine Similarity}(\\mathbf{A}, \\mathbf{B}) = \\frac{32}{3.74 \\cdot 8.77} \\approx \\frac{32}{32.8} \\approx 0.97 $$\n",
    "\n",
    "### 2.4 Cosine Distance\n",
    "Cosine distance is derived from cosine similarity and is defined as\n",
    "\n",
    "$$ cosine distance = 1 - cosine similarity $$\n",
    "\n",
    "Cosine distance is useful when you want a measure where 0 indicates high similarity and 1 indicates high dissimilarity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62075a8d",
   "metadata": {},
   "source": [
    "## 3. Word Vectors with Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4fa7642a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6b8eb260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only the medium and large library contain word vectors\n",
    "NLP = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a127361b",
   "metadata": {},
   "source": [
    "These floating point numbers encode information about this Token, which the model has learned by observing the word in its context of occurrences.\n",
    "\n",
    "You can read more here [spaCy word vectors](https://spacy.io/usage/linguistic-features#vectors-similarity) and [word2vec](https://www.tensorflow.org/text/tutorials/word2vec#:~:text=word2vec%20is%20not%20a%20singular,downstream%20natural%20language%20processing%20tasks.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0e6d22b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.7032e+00,  4.1982e+00, -5.0002e+00, -1.1322e+01,  3.1702e-02,\n",
       "       -1.0255e+00, -3.0870e+00, -3.7327e+00,  5.3875e-01,  3.5679e+00,\n",
       "        6.9276e+00,  1.5793e+00,  5.1188e-01,  3.1868e+00,  6.1534e+00,\n",
       "       -4.8941e+00, -2.9959e-01, -3.6276e+00,  2.3825e+00, -1.4402e+00,\n",
       "       -4.7577e+00,  4.3607e+00, -4.9814e+00, -3.6672e+00, -1.8052e+00,\n",
       "       -2.1888e+00, -4.2875e+00,  5.5712e+00, -5.2875e+00, -1.8346e+00,\n",
       "       -2.2015e+00, -7.7091e-01, -4.8260e+00,  1.2464e+00, -1.7945e+00,\n",
       "       -8.1280e+00,  1.9994e+00,  1.1413e+00,  3.8032e+00, -2.8783e+00,\n",
       "       -4.2136e-01, -4.4177e+00,  7.7456e+00,  4.9535e+00,  1.7402e+00,\n",
       "        1.8275e-01,  2.4218e+00, -3.1496e+00, -3.8057e-02, -2.9818e+00,\n",
       "        8.3396e-01,  1.1531e+01,  3.5684e+00,  2.5970e+00, -2.8438e+00,\n",
       "        3.2755e+00,  4.5674e+00,  3.2219e+00,  3.4206e+00,  1.1200e-01,\n",
       "        1.0303e-01, -5.8396e+00,  4.6370e-01,  2.7750e+00, -5.3713e+00,\n",
       "       -5.0247e+00, -2.0212e+00,  5.8772e-01,  1.1569e+00,  1.3224e+00,\n",
       "        4.3994e+00,  2.0444e+00,  2.1343e+00, -1.9023e+00,  2.1469e+00,\n",
       "       -2.9085e+00,  4.8429e-01, -3.3544e-01,  1.4484e+00, -1.5770e+00,\n",
       "       -1.1307e+00,  2.8320e+00,  6.2041e-01,  3.7994e+00, -3.1162e-01,\n",
       "       -6.9221e+00,  7.1342e+00,  7.2441e+00, -8.9326e+00, -2.7927e+00,\n",
       "        2.6613e-01,  6.7547e-01,  6.7293e+00, -5.8127e+00,  3.1567e+00,\n",
       "       -1.0634e+00, -1.5733e+00,  1.3534e+00,  3.9218e-01, -8.7077e+00,\n",
       "        3.4229e-02,  3.3251e+00,  4.6713e+00,  1.1865e-02,  9.8345e-01,\n",
       "       -5.3206e-02, -9.1613e+00,  6.0161e+00, -2.2223e+00,  2.5015e+00,\n",
       "       -6.0702e-01, -3.6344e-02,  7.1884e+00, -1.4431e+00,  2.6156e+00,\n",
       "       -1.0148e+00,  4.1225e+00, -1.8472e+00,  4.6292e+00, -2.6506e+00,\n",
       "       -1.8937e+00,  4.1749e+00, -9.6644e+00, -2.4813e+00, -2.7637e+00,\n",
       "       -1.0624e+00,  3.5988e+00,  4.9833e+00,  6.4499e-01,  2.5784e-01,\n",
       "        9.8727e-01, -4.2485e+00,  3.4272e-01, -2.2270e+00, -1.8957e+00,\n",
       "        8.0796e-01, -2.0265e+00, -6.1828e+00, -2.2378e+00,  2.8216e+00,\n",
       "       -2.0050e+00, -3.8924e+00, -2.9364e-01, -1.6128e+00, -6.7874e-01,\n",
       "       -1.9855e+00,  1.8221e-01,  2.1575e+00,  4.9825e-01, -1.7326e+00,\n",
       "        4.7886e+00,  2.9904e+00,  8.3447e-01, -4.7417e+00,  2.4697e+00,\n",
       "        1.3751e+00,  4.5358e+00,  6.5386e-01,  5.5413e+00,  2.3963e+00,\n",
       "        1.0031e+00, -8.0664e-01, -1.4126e+00,  2.8689e+00, -8.7339e+00,\n",
       "       -2.7457e+00, -3.1805e-01, -2.4484e-01,  3.7117e+00, -1.8636e+00,\n",
       "        2.9959e-01,  6.5062e-02, -1.5682e+00,  1.5876e+00,  6.9224e-01,\n",
       "       -6.7734e+00,  3.1065e+00,  2.3973e+00, -3.5138e+00,  3.4460e+00,\n",
       "        3.4252e+00, -5.1906e+00, -6.9372e-01,  1.9435e+00, -1.5669e-01,\n",
       "        1.9710e+00,  8.7743e-01, -8.3110e+00, -4.0306e-01, -5.0165e+00,\n",
       "       -5.6309e-02,  4.9249e+00, -7.1053e+00, -5.2338e+00,  2.3535e+00,\n",
       "       -2.5255e+00, -2.7785e+00,  5.0149e+00, -2.8405e+00, -1.8614e+00,\n",
       "        2.8818e-03,  1.3281e+00,  1.0194e+00,  3.5155e+00,  2.7971e-01,\n",
       "        1.3251e+00,  1.4386e+00, -6.1719e-01, -2.6864e+00, -3.9613e+00,\n",
       "        4.5749e+00, -1.0939e+00,  1.3289e+00, -9.5484e-01, -5.4675e+00,\n",
       "        2.1607e+00,  5.0715e-01,  1.4860e-01, -4.8571e+00, -2.2213e+00,\n",
       "       -2.3498e-01, -4.2629e+00, -8.7002e-01,  3.3796e+00, -4.3989e+00,\n",
       "        6.1047e+00,  3.7927e+00, -6.0760e+00,  3.1840e+00, -8.3104e-01,\n",
       "       -5.4015e+00, -6.2916e+00,  1.2497e+00,  1.8026e+00, -3.4535e+00,\n",
       "       -2.1652e-01, -1.4958e+00,  5.7946e-01,  2.2505e+00,  2.0868e+00,\n",
       "        3.9621e-01,  1.6076e+00,  4.0635e+00, -3.4088e+00, -1.0590e+00,\n",
       "       -3.6376e+00,  2.0501e+00,  1.4785e+00, -1.8906e+00, -2.6215e-01,\n",
       "       -5.1386e+00,  3.7029e+00, -1.8151e+00, -3.2759e+00, -5.1866e+00,\n",
       "        2.5485e-01, -4.5696e+00,  1.0147e+01, -3.0195e+00, -2.4640e+00,\n",
       "        7.5459e-01, -5.6395e+00, -5.4095e+00, -2.4363e+00, -4.3922e-01,\n",
       "       -4.0911e+00, -3.5194e+00,  1.8031e+00, -1.3644e-01,  6.7990e+00,\n",
       "        5.8461e+00,  5.3452e-01,  1.1042e+00,  3.5698e+00,  4.4668e+00,\n",
       "       -2.4537e+00, -2.1832e+00,  1.5293e+00, -1.9414e+00, -8.8675e-02,\n",
       "       -1.1825e+00, -3.9996e+00,  2.8077e+00, -1.8000e+00,  4.2545e+00,\n",
       "       -1.3813e+00, -2.2921e+00,  3.7889e+00, -1.5837e+00, -7.2078e-01,\n",
       "        4.7743e+00, -3.0923e+00,  8.4709e+00,  3.0132e-01, -5.6173e+00,\n",
       "       -5.4610e-01, -4.8459e+00,  6.0303e+00, -6.9664e+00,  3.1445e+00],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vector components for the word cat\n",
    "NLP(u\"cat\").vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "211f2ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = NLP(u\"cat pet bird\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d1c16bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat   and cat   similarity 100.0%\n",
      "cat   and pet   similarity 73.0%\n",
      "cat   and bird  similarity 54.0%\n",
      "pet   and cat   similarity 73.0%\n",
      "pet   and pet   similarity 100.0%\n",
      "pet   and bird  similarity 37.0%\n",
      "bird  and cat   similarity 54.0%\n",
      "bird  and pet   similarity 37.0%\n",
      "bird  and bird  similarity 100.0%\n"
     ]
    }
   ],
   "source": [
    "# similarity for each token with every other token\n",
    "for first_token in tokens:\n",
    "    for second_token in tokens:\n",
    "        print(f\"{first_token.text:{5}} and {second_token.text:{5}} similarity {round(first_token.similarity(second_token), 2) * 100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2761b5cb",
   "metadata": {},
   "source": [
    "## 4. Vector Arithmetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1fe4bcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "10c096b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "technology = NLP.vocab[\"technology\"].vector\n",
    "computer = NLP.vocab[\"computer\"].vector\n",
    "software = NLP.vocab[\"software\"].vector\n",
    "internet = NLP.vocab[\"internet\"].vector\n",
    "innovation = NLP.vocab[\"innovation\"].vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "484bb2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_vector = technology - computer + software"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1cbdb6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_vects = []\n",
    "\n",
    "# Iterate over words in the vocabulary\n",
    "for word in NLP.vocab:\n",
    "    if word.has_vector and word.is_lower and word.is_alpha:\n",
    "        similarity = cosine_similarity([new_vector], [word.vector])[0][0]\n",
    "        similar_vects.append((word, similarity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f2119fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_vects = sorted(similar_vects, key=lambda item: -item[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "857dcab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "technology\n",
      "software\n",
      "innovation\n",
      "internet\n",
      "and\n"
     ]
    }
   ],
   "source": [
    "# top 5 similar words\n",
    "for t in similar_vects[:5]:\n",
    "    print(t[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e95b08f",
   "metadata": {},
   "source": [
    "We can extend this approach to documents. The steps will remain relatively same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b50da66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c40b3324",
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_vector(document):\n",
    "    doc = [token for token in document if token.is_stop and token.has_vector]        \n",
    "    return np.mean([token.vector for token in doc], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "8ba367f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_one = \"AI and machine learning are being rapidly transforming the technology landscape.\"\n",
    "doc_two = \"Let's eat something\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "51745f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_one = NLP(doc_one)\n",
    "doc_two = NLP(doc_two)\n",
    "\n",
    "doc_one_vect = document_vector(doc_one)\n",
    "doc_two_vect = document_vector(doc_two)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ecd5a0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity = cosine_similarity([doc_one_vect], [doc_two_vect])[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "50b378ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between doc_one and doc_two 20.0 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Similarity between doc_one and doc_two\", round(doc_one.similarity(doc_two), 2) * 100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec378956",
   "metadata": {},
   "source": [
    "## 5. Sentiment Analysis\n",
    "Sentiment analysis is the process of determining the emotional tone behind a piece of text. It can be used to identify the sentiment expressed in reviews, social media posts, or any other text data.\n",
    "\n",
    "**Approaches to sentiment analysis**\n",
    "\n",
    "### 5.1 Lexicon-Based Approach\n",
    "- Uses predefined dictionaries of words with associated sentiment scores.\n",
    "- Words in the text are matched against these dictionaries, and the sentiment score of the text is calculated based on the matched words.\n",
    "\n",
    "### 5.2 Machine Learning Approach\n",
    "- Uses supervised learning techniques where a model is trained on a labeled dataset containing text and corresponding sentiment labels.\n",
    "- Common classifiers include Naive Bayes, Support Vector Machines (SVM), and deep learning models like Recurrent Neural Networks (RNN) and Transformers.\n",
    "\n",
    "### 5.3 Hybrid Approach\n",
    "- Combines both lexicon-based and machine learning methods to leverage the strengths of both."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e07066c",
   "metadata": {},
   "source": [
    "### 5.4 Sentiment Analysis using VADER\n",
    "(Valence Aware Dictionary and sEntiment Reasoner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "026ab4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "aa3adc20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to C:\\Users\\AR-\n",
      "[nltk_data]     LABS\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"vader_lexicon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "26e6c6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "feccd113",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentiment_using_vader(text):\n",
    "    doc = NLP(text)\n",
    "    sentiment_scores = analyzer.polarity_scores(doc.text)\n",
    "    \n",
    "    return sentiment_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "aed105ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_one = \"This product is fine, I like it.\"\n",
    "doc_two = \"Man what a mess! this thing is terrible.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "ce3c0420",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_one_sent = analyze_sentiment_using_vader(doc_one)\n",
    "doc_two_sent = analyze_sentiment_using_vader(doc_two)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "3d872452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First document sentiment:  {'neg': 0.0, 'neu': 0.482, 'pos': 0.518, 'compound': 0.5106}\n",
      "Second document sentiment:  {'neg': 0.541, 'neu': 0.459, 'pos': 0.0, 'compound': -0.7088}\n"
     ]
    }
   ],
   "source": [
    "print(\"First document sentiment: \", doc_one_sent)\n",
    "print(\"Second document sentiment: \", doc_two_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52686e3d",
   "metadata": {},
   "source": [
    "As we can see, the first document got positive score of 0.518 and neutral score of 0.482.\n",
    "\n",
    "While the second document got negative score of 0.541 and neutral score of 0.459"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69befb91",
   "metadata": {},
   "source": [
    "### 5.5 Sentiment Analysis using spaCy\n",
    "spaCy enables sentiment analysis by utilizing the [TextBlob](https://github.com/sloria/TextBlob) library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "5b7a0e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-import to clean things up\n",
    "import spacy\n",
    "from spacytextblob.spacytextblob import SpacyTextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "e559ffd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "NLP = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "7a092ad7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacytextblob.spacytextblob.SpacyTextBlob at 0x23062a17350>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NLP.add_pipe(\"spacytextblob\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "1d73dbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = NLP(\"What an amazing day it is!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "d5f99af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75.0% positive\n"
     ]
    }
   ],
   "source": [
    "polarity = doc._.blob.polarity\n",
    "print(f\"{round(polarity, 2) * 100}% {'positive' if polarity > 0 else 'negative' if polarity < 0 else 'neutral'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "10037784",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = NLP(\"Horrible mess it is mate!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "85d8321e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-61.0% negative\n"
     ]
    }
   ],
   "source": [
    "polarity = doc._.blob.polarity\n",
    "print(f\"{round(polarity, 2) * 100}% {'positive' if polarity > 0 else 'negative' if polarity < 0 else 'neutral'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac18bfb2",
   "metadata": {},
   "source": [
    "### 5.6 Sentiment Analysis using Machine Learning\n",
    "We can train machine learning algorithms such as Naive Bayes or Support Vector Classifier to do the sentiment analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8494fb2a",
   "metadata": {},
   "source": [
    "Datasets have been downloaded from here [Datasets](https://archive.ics.uci.edu/dataset/331/sentiment+labelled+sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "0f9323b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "2c599730",
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_reviews = pd.read_csv(\n",
    "    \"./sentiment labelled sentences/yelp_labelled.txt\",\n",
    "    sep=\"\\t\",\n",
    "    header=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "be9c1e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"Review\", \"Label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "a516cbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_reviews.columns = [\"Review\", \"Label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "6ba7b7f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wow... Loved this place.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crust is not good.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Not tasty and the texture was just nasty.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stopped by during the late May bank holiday of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The selection on the menu was great and so wer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Label\n",
       "0                           Wow... Loved this place.      1\n",
       "1                                 Crust is not good.      0\n",
       "2          Not tasty and the texture was just nasty.      0\n",
       "3  Stopped by during the late May bank holiday of...      1\n",
       "4  The selection on the menu was great and so wer...      1"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp_reviews.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "8fc12f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_reviews = pd.read_csv(\n",
    "    \"./sentiment labelled sentences/amazon_cells_labelled.txt\",\n",
    "    sep=\"\\t\",\n",
    "    header=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "87d3cd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_reviews.columns = [\"Review\", \"Label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "66c88cba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>So there is no way for me to plug it in here i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Good case, Excellent value.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Great for the jawbone.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tied to charger for conversations lasting more...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The mic is great.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Label\n",
       "0  So there is no way for me to plug it in here i...      0\n",
       "1                        Good case, Excellent value.      1\n",
       "2                             Great for the jawbone.      1\n",
       "3  Tied to charger for conversations lasting more...      0\n",
       "4                                  The mic is great.      1"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amazon_reviews.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "8e84b569",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_reviews = pd.read_csv(\n",
    "    \"./sentiment labelled sentences/imdb_labelled.txt\",\n",
    "    sep=\"\\t\",\n",
    "    header=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "4ec288f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_reviews.columns = [\"Review\", \"Label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "13832b68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A very, very, very slow-moving, aimless movie ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Not sure who was more lost - the flat characte...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Attempting artiness with black &amp; white and cle...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Very little music or anything to speak of.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The best scene in the movie was when Gerardo i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Label\n",
       "0  A very, very, very slow-moving, aimless movie ...      0\n",
       "1  Not sure who was more lost - the flat characte...      0\n",
       "2  Attempting artiness with black & white and cle...      0\n",
       "3       Very little music or anything to speak of.        0\n",
       "4  The best scene in the movie was when Gerardo i...      1"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_reviews.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "73ef77b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2)\n",
      "(1000, 2)\n",
      "(748, 2)\n"
     ]
    }
   ],
   "source": [
    "print(yelp_reviews.shape)\n",
    "print(amazon_reviews.shape)\n",
    "print(imdb_reviews.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "f52911e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([yelp_reviews, amazon_reviews, imdb_reviews], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "de3427b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wow... Loved this place.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crust is not good.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Not tasty and the texture was just nasty.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stopped by during the late May bank holiday of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The selection on the menu was great and so wer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Label\n",
       "0                           Wow... Loved this place.      1\n",
       "1                                 Crust is not good.      0\n",
       "2          Not tasty and the texture was just nasty.      0\n",
       "3  Stopped by during the late May bank holiday of...      1\n",
       "4  The selection on the menu was great and so wer...      1"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "ac81d2e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2748, 2)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "95a19dd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Review    0\n",
       "Label     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "f8262cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[\"Review\"]\n",
    "y = df[\"Label\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "439323e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_pipeline = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer()),\n",
    "    (\"clf\", LinearSVC(dual=False))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "00faa5b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;tfidf&#x27;, TfidfVectorizer()), (&#x27;clf&#x27;, LinearSVC(dual=False))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;tfidf&#x27;, TfidfVectorizer()), (&#x27;clf&#x27;, LinearSVC(dual=False))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearSVC</label><div class=\"sk-toggleable__content\"><pre>LinearSVC(dual=False)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('tfidf', TfidfVectorizer()), ('clf', LinearSVC(dual=False))])"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "8f059c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = classifier_pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "74438b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.82      0.83       435\n",
      "           1       0.81      0.83      0.82       390\n",
      "\n",
      "    accuracy                           0.82       825\n",
      "   macro avg       0.82      0.82      0.82       825\n",
      "weighted avg       0.82      0.82      0.82       825\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "6b6b29b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[358  77]\n",
      " [ 68 322]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "2a502bd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_pipeline.predict([\"Man you are looking amazing!\"])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db268f7",
   "metadata": {},
   "source": [
    "**1** is POSITIVE\n",
    "**0** is NEGATIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "3c8dbbc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_pipeline.predict([\"Their service is terrible\"])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e0e45b",
   "metadata": {},
   "source": [
    "## The end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
